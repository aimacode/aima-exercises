<!DOCTYPE html>
<html>
  <head>
    <title>Learning Probabilistic Models –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Learning Probabilistic Models" />
    <meta property="twitter:title" content="Learning Probabilistic Models" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="https://code.jquery.com/jquery-3.3.1.js"></script>
    <script src="//www.gstatic.com/firebasejs/5.0.4/firebase.js"></script>
    <script type="text/javascript" src="//aima-exercises.firebaseapp.com/config.js"></script>
<!--     <script src="//http://www.gstatic.com/firebasejs/5.0.4/firebase-firestore.js"></script>
 -->
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
<!--     <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 -->
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <input type="checkbox" id="toggleheader">
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <center>
            <h1>Aritificial Intelligence: A Modern Approach</h1>
            <h3>Stuart J. Russell and Peter Norvig</h3>
          </center>
        </header>
      </div>
    </div>
    <input type="checkbox" id="toggletoc">
    <div class="toc">
      <div>Table of Contents</div>
      <ul>
	<li>
		<span>Part &#x2160; Artificial Intelligence</span>
		<ol>
			<li><a href="/intro-exercises">1. Introduction</a></li>
			<li><a href="/agents-exercises">2. Intelligent Agent</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2161; Problem-solving</span>
		<ol>
			<li><a href="/search-exercises">3. Solving Problems By Searching</a></li>
			<li><a href="/advanced-search-exercises">4. Beyond Classical Search</a></li>
			<li><a href="/game-playing-exercises">5. Adversarial Search</a></li>
			<li><a href="/csp-exercises">6. Constraint Satisfaction Problems</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2162; Knowledge, reasoning, and planning</span>
		<ol>
			<li><a href="/knowledge-logic-exercises">7. Logical Agents</a></li>
			<li><a href="/fol-exercises">8. First Order Logic</a></li>
			<li><a href="/logical-inference-exercises">9. Inference In First Order Logic</a></li>
			<li><a href="/planning-exercises">10. Classical Planning</a></li>
			<li><a href="/advanced-planning-exercises">11. Planning And Acting In The Real World</a></li>
			<li><a href="/kr-exercises">12. Knowledge Representation</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2163; Uncertain knowledge and reasoning</span>
		<ol>
			<li><a href="/probability-exercises">13. Quantifying Uncertainity</a></li>
			<li><a href="/bayes-nets-exercises">14. Probabilistic Reasoning</a></li>
			<li><a href="/dbn-exercises">15. Probabilistic Reasoning Over Time</a></li>
			<li><a href="/decision-theory-exercises">16. Making Simple Decisions</a></li>
			<li><a href="/complex-decisions-exercises">17. Making Complex Decision</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2164; Learning</span>
		<ol>
			<li><a href="/concept-learning-exercises">18. Learning From Examples</a></li>
			<li><a href="/ilp-exercises">19. Knowledge In Learning</a></li>
			<li><a href="/bayesian-learning-exercises">20. Learning Probabilistic Models</a></li>
			<li><a href="/reinforcement-learning-exercises">21. Reinforcement Learning</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2165; Communicating, perceiving, and acting</span>
		<ol>
			<li><a href="/nlp-communicating-exercises">22. Natural Language Processing</a></li>
			<li><a href="/nlp-english-exercises">23. Natural Language For Communication</a></li>
			<li><a href="/perception-exercises">24. Perception</a></li>
			<li><a href="/robotics-exercises">25. Robotics</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2166; Conclusions</span>
		<ol>
			<li><a href="/philosophy-exercises">26. Philosophical Foundations</a></li>
			<li><a href="/#/"> Future Exercises</a></li>
		</ol>
	</li>
</ul>
    </div>
    <div id="main" role="main" class="container">
      



<ul class="breadcrumb">
<label for="toggletoc" class="toc-icon">
  <span></span>
  <span></span>
  <span></span>
</label>

   
    <li><a class="breadcrumb-text" href="/">home</a> &nbsp; </li>
   

<label for="toggleheader" class="toggleheader" title="Toggle Header">
    &#9167;
</label>
</ul>

      <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="20-learning-probabilistic-models">20. Learning Probabilistic Models</h1>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_1" data-rating="0"></i></div>
<p><a href="20-1/">Exercise 20.1 [bayes-candy-exercise]</a></p>

<p>The data used for
Figure <a href="#/">bayes-candy-figure</a> on page <a href="#/">bayes-candy-figure</a> can be
viewed as being generated by $h_5$. For each of the other four
hypotheses, generate a data set of length 100 and plot the corresponding
graphs for $P(h_i|d_1,\ldots,d_N)$ and
$P(D_{N+1}=lime|d_1,\ldots,d_N)$. Comment on
your results.</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_2" data-rating="0"></i></div>
<p><a href="20-2/">Exercise 20.2</a></p>

<p>Repeat Exercise <a href="#/">bayes-candy-exercise</a>, this time
plotting the values of
$P(D_{N+1}=lime|h_{MAP})$ and
$P(D_{N+1}=lime|h_{ML})$.</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_3" data-rating="0"></i></div>
<p><a href="20-3/">Exercise 20.3 [candy-trade-exercise]</a></p>

<p>Suppose that Ann’s utilities for cherry and
lime candies are $c_A$ and $\ell_A$, whereas Bob’s utilities are $c_B$
and $\ell_B$. (But once Ann has unwrapped a piece of candy, Bob won’t
buy it.) Presumably, if Bob likes lime candies much more than Ann, it
would be wise for Ann to sell her bag of candies once she is
sufficiently sure of its lime content. On the other hand, if Ann unwraps
too many candies in the process, the bag will be worth less. Discuss the
problem of determining the optimal point at which to sell the bag.
Determine the expected utility of the optimal procedure, given the prior
distribution from Section <a href="#/">statistical-learning-section</a>.</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_4" data-rating="0"></i></div>
<p><a href="20-4/">Exercise 20.4</a></p>

<p>Two statisticians go to the doctor and are both given the same
prognosis: A 40% chance that the problem is the deadly disease $A$, and
a 60% chance of the fatal disease $B$. Fortunately, there are anti-$A$
and anti-$B$ drugs that are inexpensive, 100% effective, and free of
side-effects. The statisticians have the choice of taking one drug,
both, or neither. What will the first statistician (an avid Bayesian)
do? How about the second statistician, who always uses the maximum
likelihood hypothesis?</p>

<p>The doctor does some research and discovers that disease $B$ actually
comes in two versions, dextro-$B$ and levo-$B$, which are equally likely
and equally treatable by the anti-$B$ drug. Now that there are three
hypotheses, what will the two statisticians do?</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_5" data-rating="0"></i></div>
<p><a href="20-5/">Exercise 20.5 [BNB-exercise]</a></p>

<p>Explain how to apply the boosting method of
Chapter <a href="#/">concept-learning-chapter</a> to naive Bayes
learning. Test the performance of the resulting algorithm on the
restaurant learning problem.</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_6" data-rating="0"></i></div>
<p><a href="20-6/">Exercise 20.6 [linear-regression-exercise]</a></p>

<p>Consider $N$ data points $(x_j,y_j)$,
where the $y_j$s are generated from the $x_j$s according to the linear
Gaussian model in
Equation (<a href="#/">linear-gaussian-likelihood-equation</a>). Find
the values of $\theta_1$, $\theta_2$, and $\sigma$ that maximize the
conditional log likelihood of the data.</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_7" data-rating="0"></i></div>
<p><a href="20-7/">Exercise 20.7 [noisy-OR-ML-exercise]</a></p>

<p>Consider the noisy-OR model for fever described
in Section <a href="#/">canonical-distribution-section</a>. Explain how
to apply maximum-likelihood learning to fit the parameters of such a
model to a set of complete data. (<em>Hint</em>: use the chain
rule for partial derivatives.)</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_8" data-rating="0"></i></div>
<p><a href="20-8/">Exercise 20.8 [beta-integration-exercise]</a></p>

<p>This exercise investigates properties of
the Beta distribution defined in
Equation (<a href="#/">beta-equation</a>).</p>

<ol>
  <li>
    <p>By integrating over the range $[0,1]$, show that the normalization
constant for the distribution $[a,b]$ is given by
$\alpha = \Gamma(a+b)/\Gamma(a)\Gamma(b)$ where $\Gamma(x)$ is the <strong>Gamma function</strong>,
defined by $\Gamma(x+1)x\cdot\Gamma(x)$ and
$\Gamma(1)1$. (For integer $x$,
$\Gamma(x+1)x!$.)</p>
  </li>
  <li>
    <p>Show that the mean is $a/(a+b)$.</p>
  </li>
  <li>
    <p>Find the mode(s) (the most likely value(s) of $\theta$).</p>
  </li>
  <li>
    <p>Describe the distribution $[\epsilon,\epsilon]$ for very
small $\epsilon$. What happens as such a distribution is updated?</p>
  </li>
</ol>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_9" data-rating="0"></i></div>
<p><a href="20-9/">Exercise 20.9 [ML-parents-exercise]</a></p>

<p>Consider an arbitrary Bayesian network, a
complete data set for that network, and the likelihood for the data set
according to the network. Give a simple proof that the likelihood of the
data cannot decrease if we add a new link to the network and recompute
the maximum-likelihood parameter values.</p>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_10" data-rating="0"></i></div>
<p><a href="20-10/">Exercise 20.10</a></p>

<p>Consider a single Boolean random variable $Y$ (the “classification”).
Let the prior probability $P(Y=true)$ be $\pi$. Let’s try to
find $\pi$, given a training set $D=(y_1,\ldots,y_N)$ with $N$
independent samples of $Y$. Furthermore, suppose $p$ of the $N$ are
positive and $n$ of the $N$ are negative.</p>

<ol>
  <li>
    <p>Write down an expression for the likelihood of $D$ (i.e., the
probability of seeing this particular sequence of examples, given a
fixed value of $\pi$) in terms of $\pi$, $p$, and $n$.</p>
  </li>
  <li>
    <p>By differentiating the log likelihood $L$, find the value of $\pi$
that maximizes the likelihood.</p>
  </li>
  <li>
    <p>Now suppose we add in $k$ Boolean random variables
$X_1, X_2,\ldots,X_k$ (the “attributes”) that describe each sample,
and suppose we assume that the attributes are conditionally
independent of each other given the goal $Y$. Draw the Bayes net
corresponding to this assumption.</p>
  </li>
  <li>
    <p>Write down the likelihood for the data including the attributes,
using the following additional notation:</p>

    <ul>
      <li>
        <p>$\alpha_i$ is $P(X_i=true | Y=true)$.</p>
      </li>
      <li>
        <p>$\beta_i$ is $P(X_i=true | Y=false)$.</p>
      </li>
      <li>
        <p>$p_i^+$ is the count of samples for which $X_i=true$
and $Y=true$.</p>
      </li>
      <li>
        <p>$n_i^+$ is the count of samples for which $X_i=false$
and $Y=true$.</p>
      </li>
      <li>
        <p>$p_i^-$ is the count of samples for which $X_i=true$
and $Y=false$.</p>
      </li>
      <li>
        <p>$n_i^-$ is the count of samples for which $X_i=false$
and $Y=false$.</p>
      </li>
    </ul>

    <p>[<em>Hint</em>: consider first the probability of seeing a
single example with specified values for $X_1, X_2,\ldots,X_k$ and
$Y$.]</p>
  </li>
  <li>
    <p>By differentiating the log likelihood $L$, find the values of
$\alpha_i$ and $\beta_i$ (in terms of the various counts) that
maximize the likelihood and say in words what these
values represent.</p>
  </li>
  <li>
    <p>Let $k = 2$, and consider a data set with 4 all four possible
examples of thexor function. Compute the maximum
likelihood estimates of $\pi$, $\alpha_1$, $\alpha_2$, $\beta_1$,
and $\beta_2$.</p>
  </li>
  <li>
    <p>Given these estimates of $\pi$, $\alpha_1$, $\alpha_2$, $\beta_1$,
and $\beta_2$, what are the posterior probabilities
$P(Y=true | x_1,x_2)$ for each example?</p>
  </li>
</ol>

<div><i class="arrow-up" data-chapter="bayesian-learning-exercises" data-exercise="ex_11" data-rating="0"></i></div>
<p><a href="20-11/">Exercise 20.11</a></p>

<p>Consider the application of EM to learn the parameters for the network
in Figure <a href="#/">mixture-networks-figure</a>(a), given the true
parameters in Equation (<a href="#/">candy-true-equation</a>).</p>

<ol>
  <li>
    <p>Explain why the EM algorithm would not work if there were just two
attributes in the model rather than three.</p>
  </li>
  <li>
    <p>Show the calculations for the first iteration of EM starting from
Equation (<a href="#/">candy-64-equation</a>).</p>
  </li>
  <li>
    <p>What happens if we start with all the parameters set to the same
value $p$? (<em>Hint</em>: you may find it helpful to
investigate this empirically before deriving the general result.)</p>
  </li>
  <li>
    <p>Write out an expression for the log likelihood of the tabulated
candy data on page <a href="#/">candy-counts-page</a> in terms of the parameters,
calculate the partial derivatives with respect to each parameter,
and investigate the nature of the fixed point reached in part (c).</p>
  </li>
</ol>


  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  

</article>

<script type="text/javascript">
var chapter  = window.location.pathname
var chapterName = String(chapter).match(/\/([^\/]*)\//, "")[1]
$.get( "https://aima-exercises.firebaseapp.com/rating/"+chapterName, function( data ) {
  console.log(data)
  $("i[data-chapter='"+chapterName+"']").each(function(index,element){
  	ex = $(element).data("exercise")
  	if(ex in data){
  		console.log(data[ex])
  		$(element).attr("data-rating",data[ex])
  	}
  })
});

$(document).on('click',"i[data-chapter]",function(e){
	ele = $(e.target)
	exerciseName = ele.data("exercise")
	$.post( "https://aima-exercises.firebaseapp.com/rating/"+chapterName+"/"+exerciseName, function( data ) {
	  console.log(data)
	  ele.attr("data-rating",data["rating"])
	});	
})


</script>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!--            Designed by <a href="http://nalinc.github.io" style="color:#2196F3">Nalin</a> &#9889; 
           Written in <a href="#/" style="color: #2a932a">Markdown</a> &#9889; 
           Powered by <a href="http://jekyllrb.com" style="color: #d73838">Jekyll</a>    -->
          <!-- 











 -->
        </footer>
      </div>
    </div>

    


    <script type="text/javascript">
      // firestore =firebase.firestore();
      // function rateExercise(e){
      //   console.log(e.target)
      //   chapterLabel = $(e.target).data("chapter")
      //   exerciseLabel = $(e.target).data("exercise")
      //   docRef = firestore.collection("rating").doc(chapterLabel)
      //   score = 0
      //   docRef.get().then(function(doc){
      //     if (doc && doc.exists){
      //       myData = doc.data()
      //      // score = myData
      //       console.log(myData)
      //       if(exerciseLabel in myData){
      //         myData[exerciseLabel] += 1
      //       }else{
      //         myData[exerciseLabel] = 1
      //       }
      //      // $(e.target).data("rating",score);
      //       docRef.set(myData).then(function(){
      //         console.log("status saved")
      //         console.log(myData[exerciseLabel])
      //         $(e.target).attr("data-rating",myData[exerciseLabel]);
      //       })
      //     }
      //   })
      // }
      // getRealTimeUpdates = function(){
      //   docRef = firestore.collection("rating").doc("intro-exercises");
      //   docRef.onSnapshot(function(doc){
      //     if (doc && doc.exists){
      //       myData = doc.data()
      //       for(key in myData){
      //         console.log(key)  
      //         // $(e.target).attr("data-rating",myData[exerciseLabel]);
      //       }
      //     }
      //   })
      // }

      // getRealTimeUpdates()
      // $(document).on("click",".arrow-up", rateExercise)
    </script>
  </body>
</html>
