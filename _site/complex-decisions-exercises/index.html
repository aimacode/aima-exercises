<!DOCTYPE html>
<html lang="en-us">
  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title>
    
      Making complex Decisions &middot; AIMA Exercises 
    
  </title>
  <!-- CSS -->
  <link rel="stylesheet" href="/aima-exercises/public/css/poole.css">
  <link rel="stylesheet" href="/aima-exercises/public/css/syntax.css">
  <link rel="stylesheet" href="/aima-exercises/public/css/lanyon.css">
  <link rel="stylesheet" href="/aima-exercises/public/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

      <!-- Bootstrap CSS -->
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/aima-exercises/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/aima-exercises/public/aima_logo.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <body>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Artificial Intelligence : A Modern Approach</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/aima-exercises/">Home</a>
    <span class="sidebar-nav-item">Part - I Artificial Intelligence</span>
  <a class="sidebar-nav-item" href="/aima-exercises/intro-exercises/">Chapter 1 - Introduction</a>
  <a class="sidebar-nav-item" href="/aima-exercises/agents-exercises/">Chapter 2 - Intelligent Agents</a>
  <span class="sidebar-nav-item">Part - II Problem Solving</span>
  <a class="sidebar-nav-item" href="/aima-exercises/search-exercises/">Chapter 3 - Solving Problems By Searching</a>
  <a class="sidebar-nav-item" href="/aima-exercises/advanced-search-exercises">Chapter 4 - Beyond Classical Search</a>
  <a class="sidebar-nav-item" href="/aima-exercises/game-playing-exercises">Chapter 5 - Adversarial Search</a>
  <a class="sidebar-nav-item" href="/aima-exercises/csp-exercises">Chapter 6 - Constraint Satisfaction Problems</a>
  <span class="sidebar-nav-item">Part - III Knowledge, Reasoning and Planning</span>
  <a class="sidebar-nav-item" href="/aima-exercises/knowledge-logic-exercises">Chapter 7 - Logical Agents</a>
  <a class="sidebar-nav-item" href="/aima-exercises/fol-exercises">Chapter 8 - First Order Logic</a>
  <a class="sidebar-nav-item" href="/aima-exercises/logical-inference-exercises">Chapter 9 - Inference in First Order Logic</a>
  <a class="sidebar-nav-item" href="/aima-exercises/planning-exercises">Chapter 10 - Classical Planning</a>
  <a class="sidebar-nav-item" href="/aima-exercises/advanced-planning-exercises">Chapter 11 - Planning and Acting in Real Life</a>
  <a class="sidebar-nav-item" href="/aima-exercises/kr-exercises">Chapter 12 - Knowledge Representation</a>
  <span class="sidebar-nav-item">Part - IV Uncertaing Knowledge and Reasoning</span>
  <a class="sidebar-nav-item" href="/aima-exercises/probability-exercises">Chapter 13 - Quantifying Uncertainty</a>
  <a class="sidebar-nav-item" href="/aima-exercises/bayes-nets-exercises">Chapter 14 - Probabilistic Reasoning</a>
  <a class="sidebar-nav-item" href="/aima-exercises/dbn-exercises">Chapter 15 - Probabilistic Reasoning Over Time</a>
  <a class="sidebar-nav-item" href="/aima-exercises/decision-theory-exercises">Chapter 16 - Making-Simple Decisions</a>
  <a class="sidebar-nav-item" href="/aima-exercises/complex-decisions-exercises">Chapter 17 - Making Complex Decisions</a>
  <span class="sidebar-nav-item">Part - V Lerning</span>
  <a class="sidebar-nav-item" href="/aima-exercises/concept-learning-exercises">Chapter 18 - Learning From Examples</a>
  <a class="sidebar-nav-item" href="/aima-exercises/ilp-exercises">Chapter 19 - Knowledge In Learning</a>
  <a class="sidebar-nav-item" href="/aima-exercises/bayesian-learning-exercises">Chapter 20 - Learning Probabilistic Models</a>
  <a class="sidebar-nav-item" href="/aima-exercises/reinforcement-learning-exercises">Chapter 21 - Reinforcement Learning</a>
  <span class="sidebar-nav-item">Part - VI Communicating, Perceiving and Acting</span>
  <a class="sidebar-nav-item" href="/aima-exercises/nlp-communicating-exercises">Chapter 22 - Natural Language Processing</a>
  <a class="sidebar-nav-item" href="/aima-exercises/nlp-english-exercises">Chapter 23 - Natural Language For Communication</a>
  <a class="sidebar-nav-item" href="/aima-exercises/perception-exercises">Chapter 24 - Perception</a>
  <a class="sidebar-nav-item" href="/aima-exercises/robotics-exercises">Chapter 25 - Robotics</a>
  <span class="sidebar-nav-item">Part - VII Conclusions</span>
  <a class="sidebar-nav-item" href="/aima-exercises/philosophy-exercises">Chapter 26 - Philosophical Foundations</a>
  <a class="sidebar-nav-item" href="/aima-exercises/#/">Chapter 27 - AI The Present And Future</a>
    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2019. All rights reserved.
    </p>
  </div>
</div>

    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/aima-exercises/" title="Home">Artificial Intelligence</a>
            <small>AIMA Exercises </small>
          </h3>
          <br>
          <center>
            <form class="form-inline active-pink-3 active-pink-4" action="/aima-exercises/search" id="site_search" autocomplete="off" method="GET">
              <i class="fas fa-search" aria-hidden="true"></i>
            <input class="form-control form-control-sm ml-3 w-75" type="text" placeholder="Search within AIMA Exercises" aria-label="Search" name="query">
            <input type="submit" value="Go!" class="search-btn">
            </form>
            <br>
            </center>
            



<ul class="breadcrumbb" id="bbreadcrumb">

  <label for="toggletoc" class="toc-icon">
    <span></span>
    <span></span>
    <span></span>
  </label>

   
   
    <li><a class="breadcrumb-text" href="/aima-exercises/"><i class="fa fa-home"></i></a>  </li>
   


</ul>

      </div>
    </div>
      <div class="container content">
        <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="17-making-complex-decisions">17. Making Complex Decisions</h1>

<div class="card">
<div class="card-header p-2">
<a href="ex_1/" class="p-2">Exercise 1 (mdp-model-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex1');" href="#"><i id="ch17ex1" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.1);" href="#"><i id="ch17ex1" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

For the $4\times 3$ world shown in
Figure <a class="insideBookFigRef" target="_blank" href="https://aimacode.github.io/aima-exercises/figures/sequential-decision-world-figure.png">sequential-decision-world-figure</a>., calculate
which squares can be reached from (1,1) by the action sequence
$[{Up},{Up},{Right},{Right},{Right}]$ and with what
probabilities. Explain how this computation is related to the prediction
task (see Section <a href="#">general-filtering-section</a> for a
hidden Markov model.
</p>
</div>
</div>
<p><br /></p>

<div class="card">
<div class="card-header p-2">
<a href="ex_2/" class="p-2">Exercise 2 (mdp-model-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex2');" href="#"><i id="ch17ex2" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.2);" href="#"><i id="ch17ex2" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

For the $4\times 3$ world shown in
Figure <a class="insideBookFigRef" target="_blank" href="https://aimacode.github.io/aima-exercises/figures/sequential-decision-world-figure.png">sequential-decision-world-figure</a>, calculate
which squares can be reached from (1,1) by the action sequence
$[{Right},{Right},{Right},{Up},{Up}]$ and with what
probabilities. Explain how this computation is related to the prediction
task (see Section <a class="sectionRef" title="" href="#">general-filtering-section</a>) for a
hidden Markov model.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_3/" class="p-2">Exercise 3 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex3');" href="#"><i id="ch17ex3" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.3);" href="#"><i id="ch17ex3" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Select a specific member of the set of policies that are optimal for
$R(s)&gt;0$ as shown in
Figure <a class="insideBookFigRef" target="_blank" href="https://aimacode.github.io/aima-exercises/figures/sequential-decision-policies-figure.png">sequential-decision-policies-figure</a>(b), and
calculate the fraction of time the agent spends in each state, in the
limit, if the policy is executed forever. (<i>Hint</i>:
Construct the state-to-state transition probability matrix corresponding
to the policy and see
Exercise <a class="exerciseRef" href="/aima-exercises/dbn-exercises/ex_2/">markov-convergence-exercise</a>.)
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_4/" class="p-2">Exercise 4 (nonseparable-exercise)</a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex4');" href="#"><i id="ch17ex4" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.4);" href="#"><i id="ch17ex4" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Suppose that we define the utility of a state
sequence to be the <i>maximum</i> reward obtained in any state
in the sequence. Show that this utility function does not result in
stationary preferences between state sequences. Is it still possible to
define a utility function on states such that MEU decision making gives
optimal behavior?
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_5/" class="p-2">Exercise 5 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex5');" href="#"><i id="ch17ex5" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.5);" href="#"><i id="ch17ex5" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Can any finite search problem be translated exactly into a Markov
decision problem such that an optimal solution of the latter is also an
optimal solution of the former? If so, explain <i>precisely</i>
how to translate the problem and how to translate the solution back; if
not, explain <i>precisely</i> why not (i.e., give a
counterexample).
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_6/" class="p-2">Exercise 6 (reward-equivalence-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex6');" href="#"><i id="ch17ex6" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.6);" href="#"><i id="ch17ex6" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Sometimes MDPs are formulated with a
reward function $R(s,a)$ that depends on the action taken or with a
reward function $R(s,a,s')$ that also depends on the outcome state.<br />

1.  Write the Bellman equations for these formulations.<br />

2.  Show how an MDP with reward function $R(s,a,s')$ can be transformed
    into a different MDP with reward function $R(s,a)$, such that
    optimal policies in the new MDP correspond exactly to optimal
    policies in the original MDP.<br />

3.  Now do the same to convert MDPs with $R(s,a)$ into MDPs with $R(s)$.<br />
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_7/" class="p-2">Exercise 7 (threshold-cost-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex7');" href="#"><i id="ch17ex7" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.7);" href="#"><i id="ch17ex7" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

For the environment shown in
Figure <a class="insideBookFigRef" target="_blank" href="https://aimacode.github.io/aima-exercises/figures/sequential-decision-world-figure.png">sequential-decision-world-figure</a>, find all the
threshold values for $R(s)$ such that the optimal policy changes when
the threshold is crossed. You will need a way to calculate the optimal
policy and its value for fixed $R(s)$. (<i>Hint</i>: Prove that
the value of any fixed policy varies linearly with $R(s)$.)
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_8/" class="p-2">Exercise 8 (vi-contraction-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex8');" href="#"><i id="ch17ex8" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.8);" href="#"><i id="ch17ex8" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Equation (<a class="equationRef" title="" href="#">vi-contraction-equation</a>) on
page <a class="pageRef" title="" href="#">vi-contraction-equation</a> states that the Bellman operator is a contraction.<br />

1.  Show that, for any functions $f$ and $g$,
    $$|\max_a f(a) - \max_a g(a)| \leq \max_a |f(a) - g(a)|\ .$$<br />

2.  Write out an expression for $$|(B\,U_i - B\,U'_i)(s)|$$ and then apply
    the result from (1) to complete the proof that the Bellman operator
    is a contraction.<br />
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_9/" class="p-2">Exercise 9 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex9');" href="#"><i id="ch17ex9" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.9);" href="#"><i id="ch17ex9" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

This exercise considers two-player MDPs that correspond to zero-sum,
turn-taking games like those in
Chapter <a class="chapterRef" href="/aima-exercises/game-playing-exercises/">game-playing-chapter</a>. Let the players be $A$
and $B$, and let $R(s)$ be the reward for player $A$ in state $s$. (The
reward for $B$ is always equal and opposite.)<br />

1.  Let $U_A(s)$ be the utility of state $s$ when it is $A$’s turn to
    move in $s$, and let $U_B(s)$ be the utility of state $s$ when it is
    $B$’s turn to move in $s$. All rewards and utilities are calculated
    from $A$’s point of view (just as in a minimax game tree). Write
    down Bellman equations defining $U_A(s)$ and $U_B(s)$.<br />

2.  Explain how to do two-player value iteration with these equations,
    and define a suitable termination criterion.<br />

3.  Consider the game described in
    Figure <a class="insideBookFigRef" target="_blank" href="https://aimacode.github.io/aima-exercises/figures/line-game4-figure.png">line-game4-figure</a> on page <a class="pageRef" id="pageref" title="" href="#">line-game4-figure</a>.
    Draw the state space (rather than the game tree), showing the moves
    by $A$ as solid lines and moves by $B$ as dashed lines. Mark each
    state with $R(s)$. You will find it helpful to arrange the states
    $(s_A,s_B)$ on a two-dimensional grid, using $s_A$ and $s_B$ as
    “coordinates.”<br />

4.  Now apply two-player value iteration to solve this game, and derive
    the optimal policy.<br />


    <figure>
      <img src="https://aimacode.github.io/aima-exercises/figures/grid-mdp-figure.svg" alt="grid-mdp-figure" id="grid-mdp-figure" style="width:100%" />
      <figcaption><center><b>(a) $3 \times 3$ world for Exercise <a href="#">3x3-mdp-exercise</a>. The reward for each state is indicated. The upper right square is a terminal state. (b) $101 \times 3$ world for Exercise <a href="#">101x3-mdp-exercise</a> (omitting 93 identical columns in the middle).
      The start state has reward 0.</b></center></figcaption>
    </figure>
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_10/" class="p-2">Exercise 10 (3x3-mdp-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex10');" href="#"><i id="ch17ex10" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.10);" href="#"><i id="ch17ex10" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">Consider the $3 \times 3$ world shown in
Figure <a class="insideExercisesFigRef" href="#grid-mdp-figure">grid-mdp-figure</a>(a). The transition model is the
same as in the $4\times 3$
Figure <a class="insideBookFigRef" id="insidebookfigref" target="_blank" href="https://aimacode.github.io/aima-exercises/figures/sequential-decision-world-figure.png">sequential-decision-world-figure</a>: 80% of the
time the agent goes in the direction it selects; the rest of the time it
moves at right angles to the intended direction.<br />

Implement value iteration for this world for each value of $r$ below.
Use discounted rewards with a discount factor of 0.99. Show the policy
obtained in each case. Explain intuitively why the value of $r$ leads to
each policy.<br />

1.  $r = -100$<br />

2.  $r = -3$<br />

3.  $r = 0$<br />

4.  $r = +3$<br />
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_11/" class="p-2">Exercise 11 (101x3-mdp-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex11');" href="#"><i id="ch17ex11" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.11);" href="#"><i id="ch17ex11" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Consider the $101 \times 3$ world shown in
Figure <a class="insideExercisesFigRef" href="#grid-mdp-figure">grid-mdp-figure</a>(b). In the start state the agent
has a choice of two deterministic actions, <i>Up</i> or
<i>Down</i>, but in the other states the agent has one
deterministic action, <i>Right</i>. Assuming a discounted reward
function, for what values of the discount $\gamma$ should the agent
choose <i>Up</i> and for which <i>Down</i>? Compute the
utility of each action as a function of $\gamma$. (Note that this simple
example actually reflects many real-world situations in which one must
weigh the value of an immediate action versus the potential continual
long-term consequences, such as choosing to dump pollutants into a
lake.)
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_12/" class="p-2">Exercise 12 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex12');" href="#"><i id="ch17ex12" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.12);" href="#"><i id="ch17ex12" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Consider an undiscounted MDP having three states, (1, 2, 3), with
rewards $-1$, $-2$, $0$, respectively. State 3 is a terminal state. In
states 1 and 2 there are two possible actions: $a$ and $b$. The
transition model is as follows:<br />

-   In state 1, action $a$ moves the agent to state 2 with probability
    0.8 and makes the agent stay put with probability 0.2.<br />

-   In state 2, action $a$ moves the agent to state 1 with probability
    0.8 and makes the agent stay put with probability 0.2.<br />

-   In either state 1 or state 2, action $b$ moves the agent to state 3
    with probability 0.1 and makes the agent stay put with
    probability 0.9.<br />

Answer the following questions:<br />

1.  What can be determined <i>qualitatively</i> about the
    optimal policy in states 1 and 2?<br />

2.  Apply policy iteration, showing each step in full, to determine the
    optimal policy and the values of states 1 and 2. Assume that the
    initial policy has action $b$ in both states.<br />

3.  What happens to policy iteration if the initial policy has action
    $a$ in both states? Does discounting help? Does the optimal policy
    depend on the discount factor?<br />
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_13/" class="p-2">Exercise 13 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex13');" href="#"><i id="ch17ex13" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.13);" href="#"><i id="ch17ex13" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Consider the $4\times 3$ world shown in
Figure <a class="insideBookFigRef" target="_blank" href="https://aimacode.github.io/aima-exercises/figures/sequential-decision-world-figure.png">sequential-decision-world-figure</a><br />.

1.  Implement an environment simulator for this environment, such that
    the specific geography of the environment is easily altered. Some
    code for doing this is already in the online code repository.<br />

2.  Create an agent that uses policy iteration, and measure its
    performance in the environment simulator from various
    starting states. Perform several experiments from each starting
    state, and compare the average total reward received per run with
    the utility of the state, as determined by your algorithm.<br />

3.  Experiment with increasing the size of the environment. How does the
    run time for policy iteration vary with the size of the environment?<br />
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_14/" class="p-2">Exercise 14 (policy-loss-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex14');" href="#"><i id="ch17ex14" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.14);" href="#"><i id="ch17ex14" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

How can the value determination algorithm be
used to calculate the expected loss experienced by an agent using a
given set of utility estimates ${U}$ and an estimated
model ${P}$, compared with an agent using correct values?
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_15/" class="p-2">Exercise 15 (4x3-pomdp-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex15');" href="#"><i id="ch17ex15" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.15);" href="#"><i id="ch17ex15" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Let the initial belief state $b_0$ for the
$4\times 3$ POMDP on page <a class="pageRef" title="" href="#">4x3-pomdp-page</a> be the uniform distribution
over the nonterminal states, i.e.,
$&lt; \frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},0,0 &gt;$.
Calculate the exact belief state $b_1$ after the agent moves and its
sensor reports 1 adjacent wall. Also calculate $b_2$ assuming that the
same thing happens again.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_16/" class="p-2">Exercise 16 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex16');" href="#"><i id="ch17ex16" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.16);" href="#"><i id="ch17ex16" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

What is the time complexity of $d$ steps of POMDP value iteration for a
sensorless environment?
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_17/" class="p-2">Exercise 17 (2state-pomdp-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex17');" href="#"><i id="ch17ex17" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.17);" href="#"><i id="ch17ex17" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Consider a version of the two-state POMDP on
page <a class="pageRef" title="" href="#">2state-pomdp-page</a> in which the sensor is 90% reliable in state 0 but
provides no information in state 1 (that is, it reports 0 or 1 with
equal probability). Analyze, either qualitatively or quantitatively, the
utility function and the optimal policy for this problem.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_18/" class="p-2">Exercise 18 (dominant-equilibrium-exercise) </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex18');" href="#"><i id="ch17ex18" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.18);" href="#"><i id="ch17ex18" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Show that a dominant strategy
equilibrium is a Nash equilibrium, but not vice versa.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_19/" class="p-2">Exercise 19 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex19');" href="#"><i id="ch17ex19" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.19);" href="#"><i id="ch17ex19" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

In the children’s game of rock–paper–scissors each player reveals at the
same time a choice of rock, paper, or scissors. Paper wraps rock, rock
blunts scissors, and scissors cut paper. In the extended version
rock–paper–scissors–fire–water, fire beats rock, paper, and scissors;
rock, paper, and scissors beat water; and water beats fire. Write out
the payoff matrix and find a mixed-strategy solution to this game.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_20/" class="p-2">Exercise 20 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex20');" href="#"><i id="ch17ex20" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.20);" href="#"><i id="ch17ex20" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Solve the game of <i>three</i>-finger Morra.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_21/" class="p-2">Exercise 21 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex21');" href="#"><i id="ch17ex21" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.21);" href="#"><i id="ch17ex21" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

In the <i>Prisoner’s Dilemma</i>, consider the case where after
each round, Alice and Bob have probability $X$ meeting again. Suppose
both players choose the perpetual punishment strategy (where each will
choose ${refuse}$ unless the other player has ever played
${testify}$). Assume neither player has played ${testify}$ thus far.
What is the expected future total payoff for choosing to ${testify}$
versus ${refuse}$ when $X = .2$? How about when $X = .05$? For what
value of $X$ is the expected future total payoff the same whether one
chooses to ${testify}$ or ${refuse}$ in the current round?
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_22/" class="p-2">Exercise 22 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex22');" href="#"><i id="ch17ex22" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.22);" href="#"><i id="ch17ex22" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

The following payoff matrix, from @Blinder:1983 by way of <a class="paperRef" title="" href="">Bernstein:1996</a>, shows a game between
politicians and the Federal Reserve.<br />

$$
\begin{array} 
	{|r|r|}\hline  &amp; Fed: contract &amp; Fed: do nothing &amp; Fed: expand \\ 
	\hline
		Pol: contract &amp; F=7, P=1 &amp; F=9, P=4 &amp; F=6, P=6 \\ 
		Pol: do nothing &amp; F=8, P=2 &amp; F=5, P=5 &amp; F=4, P=9 \\ 
		Pol: expand &amp; F=3, P=3 &amp; F=2, P=7 &amp; F=1, P=8\\ 
	\hline  
\end{array}
$$

<br />
Politicians can expand or contract fiscal policy, while the Fed can
expand or contract monetary policy. (And of course either side can
choose to do nothing.) Each side also has preferences for who should do
what—neither side wants to look like the bad guys. The payoffs shown are
simply the rank orderings: 9 for first choice through 1 for last choice.
Find the Nash equilibrium of the game in pure strategies. Is this a
Pareto-optimal solution? You might wish to analyze the policies of
recent administrations in this light.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_23/" class="p-2">Exercise 23 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex23');" href="#"><i id="ch17ex23" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.23);" href="#"><i id="ch17ex23" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

A Dutch auction is similar in an English auction, but rather than
starting the bidding at a low price and increasing, in a Dutch auction
the seller starts at a high price and gradually lowers the price until
some buyer is willing to accept that price. (If multiple bidders accept
the price, one is arbitrarily chosen as the winner.) More formally, the
seller begins with a price $p$ and gradually lowers $p$ by increments of
$d$ until at least one buyer accepts the price. Assuming all bidders act
rationally, is it true that for arbitrarily small $d$, a Dutch auction
will always result in the bidder with the highest value for the item
obtaining the item? If so, show mathematically why. If not, explain how
it may be possible for the bidder with highest value for the item not to
obtain it.
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_24/" class="p-2">Exercise 24 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex24');" href="#"><i id="ch17ex24" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.24);" href="#"><i id="ch17ex24" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Imagine an auction mechanism that is just like an ascending-bid auction,
except that at the end, the winning bidder, the one who bid $b_{max}$,
pays only $b_{max}/2$ rather than $b_{max}$. Assuming all agents are
rational, what is the expected revenue to the auctioneer for this
mechanism, compared with a standard ascending-bid auction?
</p>
</div>
</div>
<p><br /></p>
<div class="card">
<div class="card-header p-2">
<a href="ex_25/" class="p-2">Exercise 25 </a>
<button type="button" class="btn btn-dark float-right" title="Bookmark Exercise" onclick="bookmark('ch17ex25');" href="#"><i id="ch17ex25" class="fas fa-bookmark" style="color:white"></i></button>
<button type="button" class="btn btn-dark float-right" style="margin-left:10px; margin-right:10px;" title="Upvote Exercise" onclick="upvote('ex17.25);" href="#"><i id="ch17ex25" class="fas fa-thumbs-up" style="color:white"></i></button>
</div>
<div class="card-body">
<p class="card-text">

Teams in the National Hockey League historically received 2 points for
winning a game and 0 for losing. If the game is tied, an overtime period
is played; if nobody wins in overtime, the game is a tie and each team
gets 1 point. But league officials felt that teams were playing too
conservatively in overtime (to avoid a loss), and it would be more
exciting if overtime produced a winner. So in 1999 the officials
experimented in mechanism design: the rules were changed, giving a team
that loses in overtime 1 point, not 0. It is still 2 points for a win
and 1 for a tie. <br />

1.  Was hockey a zero-sum game before the rule change? After?<br />

2.  Suppose that at a certain time $t$ in a game, the home team has
    probability $p$ of winning in regulation time, probability $0.78-p$
    of losing, and probability 0.22 of going into overtime, where they
    have probability $q$ of winning, $.9-q$ of losing, and .1 of tying.
    Give equations for the expected value for the home and
    visiting teams.<br />

3.  Imagine that it were legal and ethical for the two teams to enter
    into a pact where they agree that they will skate to a tie in
    regulation time, and then both try in earnest to win in overtime.
    Under what conditions, in terms of $p$ and $q$, would it be rational
    for both teams to agree to this pact?<br />

4.  <a class="paperRef" title="" href="">Longley+Sankaran:2005</a> report that since the rule change, the percentage of games with a
    winner in overtime went up 18.2%, as desired, but the percentage of
    overtime games also went up 3.6%. What does that suggest about
    possible collusion or conservative play after the rule change?<br />
</p>
</div>
</div>
<p><br /></p>

  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  
</article>


      </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');
        document.addEventListener('click', function(e) {
          var target = e.target;
          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;
          checkbox.checked = false;
        }, false);
      })(document);
    </script>
        <script src="/aima-exercises/js/main.js"></script>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
        <script src="/aima-exercises/js/answer.js"></script>
        <script src="/aima-exercises/js/commsol.js"></script>
        <script src="/aima-exercises/js/forms.js"></script>
        <script src="/aima-exercises/js/crossref.js"></script>
        <script src="/aima-exercises/js/bookmark.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
  </body>
</html>
