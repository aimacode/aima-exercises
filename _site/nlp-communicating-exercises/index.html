<!DOCTYPE html>
<html>
  <head>
    <title>Main –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Main" />
    <meta property="twitter:title" content="Main" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="https://code.jquery.com/jquery-3.3.1.js"></script>
    <script src="//www.gstatic.com/firebasejs/5.0.4/firebase.js"></script>
    <script type="text/javascript" src="//aima-exercises.firebaseapp.com/config.js"></script>
<!--     <script src="//http://www.gstatic.com/firebasejs/5.0.4/firebase-firestore.js"></script>
 -->
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
<!--     <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 -->
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <input type="checkbox" id="toggleheader">
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <center>
            <h1>Aritificial Intelligence: A Modern Approach</h1>
            <h3>Stuart J. Russell and Peter Norvig</h3>
          </center>
        </header>
      </div>
    </div>
    <input type="checkbox" id="toggletoc">
    <div class="toc">
      <div>Table of Contents</div>
      <input type="text" class="toc_search" placeholder="Search within AIMA-Exercises">
      <ul>
	<li>
		<span>Part &#x2160; Artificial Intelligence</span>
		<ol>
			<li><a href="/intro-exercises">1. Introduction</a></li>
			<li><a href="/agents-exercises">2. Intelligent Agent</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2161; Problem-solving</span>
		<ol>
			<li><a href="/search-exercises">3. Solving Problems By Searching</a></li>
			<li><a href="/advanced-search-exercises">4. Beyond Classical Search</a></li>
			<li><a href="/game-playing-exercises">5. Adversarial Search</a></li>
			<li><a href="/csp-exercises">6. Constraint Satisfaction Problems</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2162; Knowledge, reasoning, and planning</span>
		<ol>
			<li><a href="/knowledge-logic-exercises">7. Logical Agents</a></li>
			<li><a href="/fol-exercises">8. First Order Logic</a></li>
			<li><a href="/logical-inference-exercises">9. Inference In First Order Logic</a></li>
			<li><a href="/planning-exercises">10. Classical Planning</a></li>
			<li><a href="/advanced-planning-exercises">11. Planning And Acting In The Real World</a></li>
			<li><a href="/kr-exercises">12. Knowledge Representation</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2163; Uncertain knowledge and reasoning</span>
		<ol>
			<li><a href="/probability-exercises">13. Quantifying Uncertainity</a></li>
			<li><a href="/bayes-nets-exercises">14. Probabilistic Reasoning</a></li>
			<li><a href="/dbn-exercises">15. Probabilistic Reasoning Over Time</a></li>
			<li><a href="/decision-theory-exercises">16. Making Simple Decisions</a></li>
			<li><a href="/complex-decisions-exercises">17. Making Complex Decision</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2164; Learning</span>
		<ol>
			<li><a href="/concept-learning-exercises">18. Learning From Examples</a></li>
			<li><a href="/ilp-exercises">19. Knowledge In Learning</a></li>
			<li><a href="/bayesian-learning-exercises">20. Learning Probabilistic Models</a></li>
			<li><a href="/reinforcement-learning-exercises">21. Reinforcement Learning</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2165; Communicating, perceiving, and acting</span>
		<ol>
			<li><a href="/nlp-communicating-exercises">22. Natural Language Processing</a></li>
			<li><a href="/nlp-english-exercises">23. Natural Language For Communication</a></li>
			<li><a href="/perception-exercises">24. Perception</a></li>
			<li><a href="/robotics-exercises">25. Robotics</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2166; Conclusions</span>
		<ol>
			<li><a href="/philosophy-exercises">26. Philosophical Foundations</a></li>
			<li><a href="/#/"> Future Exercises</a></li>
		</ol>
	</li>
</ul>
    </div>
    <div id="main" role="main" class="container">
      



<ul class="breadcrumb">

  <label for="toggletoc" class="toc-icon">
    <span></span>
    <span></span>
    <span></span>
  </label>

   
    <li><a class="breadcrumb-text" href="/">home</a> &nbsp; </li>
   

<label for="toggleheader" class="toggleheader" title="Toggle Header">
    &#9167;
</label>
</ul>

      <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="22-natural-language-processing">22. Natural Language Processing</h1>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_1" data-rating="0"></i></div>
<p><a href="ex_1/">Exercise 22.1</a></p>

<p>This exercise explores the quality of the $n$-gram model of language.
Find or create a monolingual corpus of 100,000 words or more. Segment it
into words, and compute the frequency of each word. How many distinct
words are there? Also count frequencies of bigrams (two consecutive
words) and trigrams (three consecutive words). Now use those frequencies
to generate language: from the unigram, bigram, and trigram models, in
turn, generate a 100-word text by making random choices according to the
frequency counts. Compare the three generated texts with actual
language. Finally, calculate the perplexity of each model.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_2" data-rating="0"></i></div>
<p><a href="ex_2/">Exercise 22.2</a></p>

<p>Write a program to do <strong>segmentation</strong> of
words without spaces. Given a string, such as the URL
“thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,”
return a list of component words: [“the,” “longest,” “list,”
$\ldots$]. This task is useful for parsing URLs, for spelling
correction when words runtogether, and for languages such as Chinese
that do not have spaces between words. It can be solved with a unigram
or bigram word model and a dynamic programming algorithm similar to the
Viterbi algorithm.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_3" data-rating="0"></i></div>
<p><a href="ex_3/">Exercise 22.3</a></p>

<p><em>Zipf’s law</em> of word distribution states the following:
Take a large corpus of text, count the frequency of every word in the
corpus, and then rank these frequencies in decreasing order. Let $f_{I}$
be the $I$th largest frequency in this list; that is, $f_{1}$ is the
frequency of the most common word (usually “the”), $f_{2}$ is the
frequency of the second most common word, and so on. Zipf’s law states
that $f_{I}$ is approximately equal to $\alpha / I$ for some constant
$\alpha$. The law tends to be highly accurate except for very small and
very large values of $I$.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_4" data-rating="0"></i></div>
<p><a href="ex_4/">Exercise 22.4</a></p>

<p>Choose a corpus of at least 20,000 words of online text, and verify
Zipf’s law experimentally. Define an error measure and find the value of
$\alpha$ where Zipf’s law best matches your experimental data. Create a
log–log graph plotting $f_{I}$ vs. $I$ and $\alpha/I$ vs. $I$. (On a
log–log graph, the function $\alpha/I$ is a straight line.) In carrying
out the experiment, be sure to eliminate any formatting tokens (e.g.,
HTML tags) and normalize upper and lower case.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_5" data-rating="0"></i></div>
<p><a href="ex_5/">Exercise 22.5</a></p>

<p>(Adapted from @Jurafsky+Martin:2000.) In this exercise you will develop a classifier for
authorship: given a text, the classifier predicts which of two candidate
authors wrote the text. Obtain samples of text from two different
authors. Separate them into training and test sets. Now train a language
model on the training set. You can choose what features to use;
$n$-grams of words or letters are the easiest, but you can add
additional features that you think may help. Then compute the
probability of the text under each language model and chose the most
probable model. Assess the accuracy of this technique. How does accuracy
change as you alter the set of features? This subfield of linguistics is
called <strong>stylometry</strong>; its successes include the identification of the author of the
disputed <em>Federalist Papers</em> @Mosteller+Wallace:1964 and
some disputed works of Shakespeare @Hope:1994. @Khmelev+Tweedie:2001 produce good results with
a simple letter bigram model.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_6" data-rating="0"></i></div>
<p><a href="ex_6/">Exercise 22.6</a></p>

<p>This exercise concerns the classification of spam email.
Create a corpus of spam email and one of non-spam mail. Examine each
corpus and decide what features appear to be useful for classification:
unigram words? bigrams? message length, sender, time of arrival? Then
train a classification algorithm (decision tree, naive Bayes, SVM,
logistic regression, or some other algorithm of your choosing) on a
training set and report its accuracy on a test set.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_7" data-rating="0"></i></div>
<p><a href="ex_7/">Exercise 22.7</a></p>

<p>Create a test set of ten queries, and pose them to three major Web
search engines. Evaluate each one for precision at 1, 3, and 10
documents. Can you explain the differences between engines?</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_8" data-rating="0"></i></div>
<p><a href="ex_8/">Exercise 22.8</a></p>

<p>Try to ascertain which of the search engines from the previous exercise
are using case folding, stemming, synonyms, and spelling correction.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_9" data-rating="0"></i></div>
<p><a href="ex_9/">Exercise 22.9</a></p>

<p>Estimate how much storage space is necessary for the index to a 100
billion-page corpus of Web pages. Show the assumptions you made.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_10" data-rating="0"></i></div>
<p><a href="ex_10/">Exercise 22.10</a></p>

<p>Write a regular expression or a short program to extract company names.
Test it on a corpus of business news articles. Report your recall and
precision.</p>

<div><i class="arrow-up loader" data-chapter="nlp-communicating-exercises" data-exercise="ex_11" data-rating="0"></i></div>
<p><a href="ex_11/">Exercise 22.11</a></p>

<p>Consider the problem of trying to evaluate the quality of an IR system
that returns a ranked list of answers (like most Web search engines).
The appropriate measure of quality depends on the presumed model of what
the searcher is trying to achieve, and what strategy she employs. For
each of the following models, propose a corresponding numeric measure.</p>

<ol>
  <li>
    <p>The searcher will look at the first twenty answers returned, with
the objective of getting as much relevant information as possible.</p>
  </li>
  <li>
    <p>The searcher needs only one relevant document, and will go down the
list until she finds the first one.</p>
  </li>
  <li>
    <p>The searcher has a fairly narrow query and is able to examine all
the answers retrieved. She wants to be sure that she has seen
everything in the document collection that is relevant to her query.
(E.g., a lawyer wants to be sure that she has found
<em>all</em> relevant precedents, and is willing to spend
considerable resources on that.)</p>
  </li>
  <li>
    <p>The searcher needs just one document relevant to the query, and can
afford to pay a research assistant for an hour’s work looking
through the results. The assistant can look through 100 retrieved
documents in an hour. The assistant will charge the searcher for the
full hour regardless of whether he finds it immediately or at the
end of the hour.</p>
  </li>
  <li>
    <p>The searcher will look through all the answers. Examining a document
has cost \$ A; finding a relevant document has value \$ B; failing
to find a relevant document has cost \$ C for each relevant
document not found.</p>
  </li>
  <li>
    <p>The searcher wants to collect as many relevant documents as
possible, but needs steady encouragement. She looks through the
documents in order. If the documents she has looked at so far are
mostly good, she will continue; otherwise, she will stop.</p>
  </li>
</ol>


  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'nalinc';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


</article>

<script type="text/javascript">
var chapter  = String('/nlp-communicating-exercises/')
var chapterName = chapter.match(/\/([^\/]*)\//, "")[1]
$.get( "https://aima-exercises.firebaseapp.com/rating/"+chapterName, function( data ) {
  console.log(data)
  $("i[data-chapter='"+chapterName+"']").each(function(index,element){
  	ex = $(element).data("exercise")
  	if(ex in data){
  		console.log(data[ex])
  		$(element).attr("data-rating",data[ex])
  	}
    $(".arrow-up").removeClass("loader")
  })
});

$(document).on('click',"i[data-chapter]",function(e){
	ele = $(e.target)
  ele.addClass("loader")
	exerciseName = ele.data("exercise")
	$.post( "https://aima-exercises.firebaseapp.com/rating/"+chapterName+"/"+exerciseName, function( data ) {
	  console.log(data)
	  ele.attr("data-rating",data["rating"])
    ele.removeClass("loader")
	});	
})


</script>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!--            Designed by <a href="http://nalinc.github.io" style="color:#2196F3">Nalin</a> &#9889; 
           Written in <a href="#/" style="color: #2a932a">Markdown</a> &#9889; 
           Powered by <a href="http://jekyllrb.com" style="color: #d73838">Jekyll</a>    -->
          <!-- 











 -->
        </footer>
      </div>
    </div>

    


    <script type="text/javascript">
      // firestore =firebase.firestore();
      // function rateExercise(e){
      //   console.log(e.target)
      //   chapterLabel = $(e.target).data("chapter")
      //   exerciseLabel = $(e.target).data("exercise")
      //   docRef = firestore.collection("rating").doc(chapterLabel)
      //   score = 0
      //   docRef.get().then(function(doc){
      //     if (doc && doc.exists){
      //       myData = doc.data()
      //      // score = myData
      //       console.log(myData)
      //       if(exerciseLabel in myData){
      //         myData[exerciseLabel] += 1
      //       }else{
      //         myData[exerciseLabel] = 1
      //       }
      //      // $(e.target).data("rating",score);
      //       docRef.set(myData).then(function(){
      //         console.log("status saved")
      //         console.log(myData[exerciseLabel])
      //         $(e.target).attr("data-rating",myData[exerciseLabel]);
      //       })
      //     }
      //   })
      // }
      // getRealTimeUpdates = function(){
      //   docRef = firestore.collection("rating").doc("intro-exercises");
      //   docRef.onSnapshot(function(doc){
      //     if (doc && doc.exists){
      //       myData = doc.data()
      //       for(key in myData){
      //         console.log(key)  
      //         // $(e.target).attr("data-rating",myData[exerciseLabel]);
      //       }
      //     }
      //   })
      // }

      // getRealTimeUpdates()
      // $(document).on("click",".arrow-up", rateExercise)
    </script>
  </body>
</html>
