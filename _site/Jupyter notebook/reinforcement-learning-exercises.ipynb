{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**21.1** Implement a passive learning agent in a simple environment, such as the\n",
    "$4\\times 3$ world. For the case of an initially unknown environment\n",
    "model, compare the learning performance of the direct utility\n",
    "estimation, TD, and ADP algorithms. Do the comparison for the optimal\n",
    "policy and for several random policies. For which do the utility\n",
    "estimates converge faster? What happens when the size of the environment\n",
    "is increased? (Try environments with and without obstacles.)\n",
    "\n",
    "**21.2** Chapter [complex-decisions-chapter](#/) defined a\n",
    "**proper policy** for an MDP as one that is\n",
    "guaranteed to reach a terminal state. Show that it is possible for a\n",
    "passive ADP agent to learn a transition model for which its policy $\\pi$\n",
    "is improper even if $\\pi$ is proper for the true MDP; with such models,\n",
    "the POLICY-EVALUATION step may fail if $\\gamma{{\\,{=}\\,}}1$. Show that this problem cannot\n",
    "arise if POLICY-EVALUATION is applied to the learned model only at the end of a trial.\n",
    "\n",
    "**21.3** \\[prioritized-sweeping-exercise\\]Starting with the passive ADP agent,\n",
    "modify it to use an approximate ADP algorithm as discussed in the text.\n",
    "Do this in two steps:\n",
    "\n",
    "1.  Implement a priority queue for adjustments to the utility estimates.\n",
    "    Whenever a state is adjusted, all of its predecessors also become\n",
    "    candidates for adjustment and should be added to the queue. The\n",
    "    queue is initialized with the state from which the most recent\n",
    "    transition took place. Allow only a fixed number of adjustments.\n",
    "\n",
    "2.  Experiment with various heuristics for ordering the priority queue,\n",
    "    examining their effect on learning rates and computation time.\n",
    "\n",
    "**21.4** The direct utility estimation method in\n",
    "Section [passive-rl-section](#/) uses distinguished terminal\n",
    "states to indicate the end of a trial. How could it be modified for\n",
    "environments with discounted rewards and no terminal states?\n",
    "\n",
    "**21.5** Write out the parameter update equations for TD learning with\n",
    "$$\\hat{U}(x,y) = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3\\,\\sqrt{(x-x_g)^2 + (y-y_g)^2}\\ .$$\n",
    "\n",
    "**21.6** Adapt the vacuum world (Chapter [agents-chapter](#/)) for\n",
    "reinforcement learning by including rewards for squares being clean.\n",
    "Make the world observable by providing suitable percepts. Now experiment\n",
    "with different reinforcement learning agents. Is function approximation\n",
    "necessary for success? What sort of approximator works for this\n",
    "application?\n",
    "\n",
    "**21.7** \\[approx-LMS-exercise\\]Implement an exploring reinforcement learning\n",
    "agent that uses direct utility estimation. Make two versions—one with a\n",
    "tabular representation and one using the function approximator in\n",
    "Equation ([4x3-linear-approx-equation](#/)). Compare their\n",
    "performance in three environments:\n",
    "\n",
    "1.  The $4\\times 3$ world described in the chapter.\n",
    "\n",
    "2.  A ${10}\\times {10}$ world with no obstacles and a +1 reward\n",
    "    at (10,10).\n",
    "\n",
    "3.  A ${10}\\times {10}$ world with no obstacles and a +1 reward\n",
    "    at (5,5).\n",
    "\n",
    "**21.8** Devise suitable features for reinforcement learning in stochastic grid\n",
    "worlds (generalizations of the $4\\times 3$ world) that contain multiple\n",
    "obstacles and multiple terminal states with rewards of $+1$ or $-1$.\n",
    "\n",
    "**21.9** Extend the standard game-playing environment\n",
    "(Chapter [game-playing-chapter](#/)) to incorporate a reward\n",
    "signal. Put two reinforcement learning agents into the environment (they\n",
    "may, of course, share the agent program) and have them play against each\n",
    "other. Apply the generalized TD update rule\n",
    "(Equation ([generalized-td-equation](#/))) to update the\n",
    "evaluation function. You might wish to start with a simple linear\n",
    "weighted evaluation function and a simple game, such as tic-tac-toe.\n",
    "\n",
    "**21.10** \\[10x10-exercise\\] Compute the true utility function and the best linear\n",
    "approximation in $x$ and $y$ (as in\n",
    "Equation ([4x3-linear-approx-equation](#/))) for the\n",
    "following environments:\n",
    "\n",
    "1.  A ${10}\\times {10}$ world with a single $+1$ terminal state\n",
    "    at (10,10).\n",
    "\n",
    "2.  As in (a), but add a $-1$ terminal state at (10,1).\n",
    "\n",
    "3.  As in (b), but add obstacles in 10 randomly selected squares.\n",
    "\n",
    "4.  As in (b), but place a wall stretching from (5,2) to (5,9).\n",
    "\n",
    "5.  As in (a), but with the terminal state at (5,5).\n",
    "\n",
    "The actions are deterministic moves in the four directions. In each\n",
    "case, compare the results using three-dimensional plots. For each\n",
    "environment, propose additional features (besides $x$ and $y$) that\n",
    "would improve the approximation and show the results.\n",
    "\n",
    "**21.11** Implement the REINFORCE and PEGASUS algorithms and apply them to the $4\\times 3$ world,\n",
    "using a policy family of your own choosing. Comment on the results.\n",
    "\n",
    "**21.12** Investigate the application of reinforcement learning ideas to the\n",
    "modeling of human and animal behavior.\n",
    "\n",
    "**21.13** Is reinforcement learning an appropriate abstract model for evolution?\n",
    "What connection exists, if any, between hardwired reward signals and\n",
    "evolutionary fitness?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
