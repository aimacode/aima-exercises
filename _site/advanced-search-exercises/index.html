<!DOCTYPE html>
<html>
  <head>
    <title>Main –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Main" />
    <meta property="twitter:title" content="Main" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="https://code.jquery.com/jquery-3.3.1.js"></script>
    <script src="//www.gstatic.com/firebasejs/5.0.4/firebase.js"></script>
    <script type="text/javascript" src="//aima-exercises.firebaseapp.com/config.js"></script>
<!--     <script src="//http://www.gstatic.com/firebasejs/5.0.4/firebase-firestore.js"></script>
 -->
    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
<!--     <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 -->
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <input type="checkbox" id="toggleheader">
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <center>
            <h1>Aritificial Intelligence: A Modern Approach</h1>
            <h3>Stuart J. Russell and Peter Norvig</h3>
          </center>
        </header>
      </div>
    </div>
    <input type="checkbox" id="toggletoc">
    <div class="toc">
      <div>Table of Contents</div>
      <form action="/search" id="site_search" autocomplete="off" method="GET">
        <input type="text" name="query" class="toc_search" placeholder="Search within AIMA Exercises">
      </form>
      <ul>
	<li>
		<span>Part &#x2160; Artificial Intelligence</span>
		<ol>
			<li><a href="/intro-exercises">1. Introduction</a></li>
			<li><a href="/agents-exercises">2. Intelligent Agent</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2161; Problem-solving</span>
		<ol>
			<li><a href="/search-exercises">3. Solving Problems By Searching</a></li>
			<li><a href="/advanced-search-exercises">4. Beyond Classical Search</a></li>
			<li><a href="/game-playing-exercises">5. Adversarial Search</a></li>
			<li><a href="/csp-exercises">6. Constraint Satisfaction Problems</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2162; Knowledge, reasoning, and planning</span>
		<ol>
			<li><a href="/knowledge-logic-exercises">7. Logical Agents</a></li>
			<li><a href="/fol-exercises">8. First Order Logic</a></li>
			<li><a href="/logical-inference-exercises">9. Inference In First Order Logic</a></li>
			<li><a href="/planning-exercises">10. Classical Planning</a></li>
			<li><a href="/advanced-planning-exercises">11. Planning And Acting In The Real World</a></li>
			<li><a href="/kr-exercises">12. Knowledge Representation</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2163; Uncertain knowledge and reasoning</span>
		<ol>
			<li><a href="/probability-exercises">13. Quantifying Uncertainity</a></li>
			<li><a href="/bayes-nets-exercises">14. Probabilistic Reasoning</a></li>
			<li><a href="/dbn-exercises">15. Probabilistic Reasoning Over Time</a></li>
			<li><a href="/decision-theory-exercises">16. Making Simple Decisions</a></li>
			<li><a href="/complex-decisions-exercises">17. Making Complex Decision</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2164; Learning</span>
		<ol>
			<li><a href="/concept-learning-exercises">18. Learning From Examples</a></li>
			<li><a href="/ilp-exercises">19. Knowledge In Learning</a></li>
			<li><a href="/bayesian-learning-exercises">20. Learning Probabilistic Models</a></li>
			<li><a href="/reinforcement-learning-exercises">21. Reinforcement Learning</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2165; Communicating, perceiving, and acting</span>
		<ol>
			<li><a href="/nlp-communicating-exercises">22. Natural Language Processing</a></li>
			<li><a href="/nlp-english-exercises">23. Natural Language For Communication</a></li>
			<li><a href="/perception-exercises">24. Perception</a></li>
			<li><a href="/robotics-exercises">25. Robotics</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2166; Conclusions</span>
		<ol>
			<li><a href="/philosophy-exercises">26. Philosophical Foundations</a></li>
			<li><a href="/#/"> Future Exercises</a></li>
		</ol>
	</li>
</ul>
    </div>
    <div id="main" role="main" class="container">
      



<ul class="breadcrumb">

  <label for="toggletoc" class="toc-icon">
    <span></span>
    <span></span>
    <span></span>
  </label>

   
    <li><a class="breadcrumb-text" href="/">home</a> &nbsp; </li>
   

<label for="toggleheader" class="toggleheader" title="Toggle Header">
    &#9167;
</label>
</ul>

      <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="4-beyond-classical-search">4. Beyond Classical Search</h1>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_1" data-rating="0"></i></div>
<p><a href="ex_1/">Exercise 4.1</a></p>

<p>Give the name of the algorithm that results from each of the following
special cases:</p>

<ol>
  <li>
    <p>Local beam search with $k = 1$.</p>
  </li>
  <li>
    <p>Local beam search with one initial state and no limit on the number
of states retained.</p>
  </li>
  <li>
    <p>Simulated annealing with $T = 0$ at all times (and omitting the
termination test).</p>
  </li>
  <li>
    <p>Simulated annealing with $T=\infty$ at all times.</p>
  </li>
  <li>
    <p>Genetic algorithm with population size $N = 1$.</p>
  </li>
</ol>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_2" data-rating="0"></i></div>
<p><a href="ex_2/">Exercise 4.2</a></p>

<p>Exercise <a href="#/">brio-exercise</a> considers the problem of
building railway tracks under the assumption that pieces fit exactly
with no slack. Now consider the real problem, in which pieces don’t fit
exactly but allow for up to 10 degrees of rotation to either side of the
“proper” alignment. Explain how to formulate the problem so it could be
solved by simulated annealing.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_3" data-rating="0"></i></div>
<p><a href="ex_3/">Exercise 4.3</a></p>

<p>In this exercise, we explore the use of local search methods to solve
TSPs of the type defined in Exercise <a href="#/">tsp-mst-exercise</a>.</p>

<ol>
  <li>
    <p>Implement and test a hill-climbing method to solve TSPs. Compare the
results with optimal solutions obtained from the A* algorithm with
the MST heuristic (Exercise <a href="#/">tsp-mst-exercise</a>).</p>
  </li>
  <li>
    <p>Repeat part (a) using a genetic algorithm instead of hill climbing.
You may want to consult @Larranaga+al:1999 for some suggestions for representations.</p>
  </li>
</ol>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_4" data-rating="0"></i></div>
<p><a href="ex_4/">Exercise 4.4 [hill-climbing-exercise]</a></p>

<p>Generate a large number of 8-puzzle and
8-queens instances and solve them (where possible) by hill climbing
(steepest-ascent and first-choice variants), hill climbing with random
restart, and simulated annealing. Measure the search cost and percentage
of solved problems and graph these against the optimal solution cost.
Comment on your results.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_5" data-rating="0"></i></div>
<p><a href="ex_5/">Exercise 4.5 [cond-plan-repeated-exercise]</a></p>

<p>The <strong>And-Or-Graph-Search</strong> algorithm in
Figure <a href="#/">and-or-graph-search-algorithm</a> checks for
repeated states only on the path from the root to the current state.
Suppose that, in addition, the algorithm were to store
<em>every</em> visited state and check against that list. (See in
Figure <a href="#/">breadth-first-search-algorithm</a> for an example.)
Determine the information that should be stored and how the algorithm
should use that information when a repeated state is found.
(<em>Hint</em>: You will need to distinguish at least between
states for which a successful subplan was constructed previously and
states for which no subplan could be found.) Explain how to use labels,
as defined in Section <a href="#/">cyclic-plan-section</a>, to avoid
having multiple copies of subplans.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_6" data-rating="0"></i></div>
<p><a href="ex_6/">Exercise 4.6 [cond-loop-exercise]</a></p>

<p>Explain precisely how to modify the <strong>And-Or-Graph-Search</strong> algorithm to
generate a cyclic plan if no acyclic plan exists. You will need to deal
with three issues: labeling the plan steps so that a cyclic plan can
point back to an earlier part of the plan, modifying <strong>Or-Search</strong> so that it
continues to look for acyclic plans after finding a cyclic plan, and
augmenting the plan representation to indicate whether a plan is cyclic.
Show how your algorithm works on (a) the slippery vacuum world, and (b)
the slippery, erratic vacuum world. You might wish to use a computer
implementation to check your results.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_7" data-rating="0"></i></div>
<p><a href="ex_7/">Exercise 4.7</a></p>

<p>In Section <a href="#/">conformant-section</a> we introduced belief
states to solve sensorless search problems. A sequence of actions solves
a sensorless problem if it maps every physical state in the initial
belief state $b$ to a goal state. Suppose the agent knows $h^*(s)$, the
true optimal cost of solving the physical state $s$ in the fully
observable problem, for every state $s$ in $b$. Find an admissible
heuristic $h(b)$ for the sensorless problem in terms of these costs, and
prove its admissibilty. Comment on the accuracy of this heuristic on the
sensorless vacuum problem of
Figure <a href="#/">vacuum2-sets-figure</a>. How well does A* perform?</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_8" data-rating="0"></i></div>
<p><a href="ex_8/">Exercise 4.8 [belief-state-superset-exercise]</a></p>

<p>This exercise explores
subset–superset relations between belief states in sensorless or
partially observable environments.</p>

<ol>
  <li>
    <p>Prove that if an action sequence is a solution for a belief state
$b$, it is also a solution for any subset of $b$. Can anything be
said about supersets of $b$?</p>
  </li>
  <li>
    <p>Explain in detail how to modify graph search for sensorless problems
to take advantage of your answers in (a).</p>
  </li>
  <li>
    <p>Explain in detail how to modify and–or search for
partially observable problems, beyond the modifications you describe
in (b).</p>
  </li>
</ol>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_9" data-rating="0"></i></div>
<p><a href="ex_9/">Exercise 4.9 [multivalued-sensorless-exercise]</a></p>

<p>On page <a href="#/">multivalued-sensorless-page</a> it was assumed
that a given action would have the same cost when executed in any
physical state within a given belief state. (This leads to a
belief-state search problem with well-defined step costs.) Now consider
what happens when the assumption does not hold. Does the notion of
optimality still make sense in this context, or does it require
modification? Consider also various possible definitions of the “cost”
of executing an action in a belief state; for example, we could use the
<em>minimum</em> of the physical costs; or the
<em>maximum</em>; or a cost <em>interval</em> with the lower
bound being the minimum cost and the upper bound being the maximum; or
just keep the set of all possible costs for that action. For each of
these, explore whether A* (with modifications if necessary) can return
optimal solutions.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_10" data-rating="0"></i></div>
<p><a href="ex_10/">Exercise 4.10 [vacuum-solvable-exercise]</a></p>

<p>Consider the sensorless version of the
erratic vacuum world. Draw the belief-state space reachable from the
initial belief state ${1,2,3,4,5,6,7,8}$, and explain why the
problem is unsolvable.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_11" data-rating="0"></i></div>
<p><a href="ex_11/">Exercise 4.11 [vacuum-solvable-exercise]</a></p>

<p>Consider the sensorless version of the
erratic vacuum world. Draw the belief-state space reachable from the
initial belief state ${ 1,3,5,7 }$, and explain why the problem
is unsolvable.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_12" data-rating="0"></i></div>
<p><a href="ex_12/">Exercise 4.12 [path-planning-agent-exercise]</a></p>

<p>We can turn the navigation problem in
Exercise <a href="#/">path-planning-exercise</a> into an environment as
follows:</p>

<ul>
  <li>
    <p>The percept will be a list of the positions, <em>relative to the
agent</em>, of the visible vertices. The percept does
<em>not</em> include the position of the robot! The robot must
learn its own position from the map; for now, you can assume that
each location has a different “view.”</p>
  </li>
  <li>
    <p>Each action will be a vector describing a straight-line path
to follow. If the path is unobstructed, the action succeeds;
otherwise, the robot stops at the point where its path first
intersects an obstacle. If the agent returns a zero motion vector
and is at the goal (which is fixed and known), then the environment
teleports the agent to a <em>random location</em> (not inside
an obstacle).</p>
  </li>
  <li>
    <p>The performance measure charges the agent 1 point for each unit of
distance traversed and awards 1000 points each time the goal
is reached.</p>
  </li>
</ul>

<ol>
  <li>
    <p>Implement this environment and a problem-solving agent for it. After
each teleportation, the agent will need to formulate a new problem,
which will involve discovering its current location.</p>
  </li>
  <li>
    <p>Document your agent’s performance (by having the agent generate
suitable commentary as it moves around) and report its performance
over 100 episodes.</p>
  </li>
  <li>
    <p>Modify the environment so that 30% of the time the agent ends up at
an unintended destination (chosen randomly from the other visible
vertices if any; otherwise, no move at all). This is a crude model
of the motion errors of a real robot. Modify the agent so that when
such an error is detected, it finds out where it is and then
constructs a plan to get back to where it was and resume the
old plan. Remember that sometimes getting back to where it was might
also fail! Show an example of the agent successfully overcoming two
successive motion errors and still reaching the goal.</p>
  </li>
  <li>
    <p>Now try two different recovery schemes after an error: (1) head for
the closest vertex on the original route; and (2) replan a route to
the goal from the new location. Compare the performance of the three
recovery schemes. Would the inclusion of search costs affect the
comparison?</p>
  </li>
  <li>
    <p>Now suppose that there are locations from which the view
is identical. (For example, suppose the world is a grid with
square obstacles.) What kind of problem does the agent now face?
What do solutions look like?</p>
  </li>
</ol>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_13" data-rating="0"></i></div>
<p><a href="ex_13/">Exercise 4.13 [online-offline-exercise]</a></p>

<p>Suppose that an agent is in a $3 \times 3$
maze environment like the one shown in
Figure <a href="#/">maze-3x3-figure</a>. The agent knows that its
initial location is (1,1), that the goal is at (3,3), and that the
actions <em>Up</em>, <em>Down</em>, <em>Left</em>, <em>Right</em> have their usual
effects unless blocked by a wall. The agent does <em>not</em> know
where the internal walls are. In any given state, the agent perceives
the set of legal actions; it can also tell whether the state is one it
has visited before.</p>

<ol>
  <li>
    <p>Explain how this online search problem can be viewed as an offline
search in belief-state space, where the initial belief state
includes all possible environment configurations. How large is the
initial belief state? How large is the space of belief states?</p>
  </li>
  <li>
    <p>How many distinct percepts are possible in the initial state?</p>
  </li>
  <li>
    <p>Describe the first few branches of a contingency plan for this
problem. How large (roughly) is the complete plan?</p>
  </li>
</ol>

<p>Notice that this contingency plan is a solution for <em>every
possible environment</em> fitting the given description. Therefore,
interleaving of search and execution is not strictly necessary even in
unknown environments.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_14" data-rating="0"></i></div>
<p><a href="ex_14/">Exercise 4.14 [online-offline-exercise]</a></p>

<p>Suppose that an agent is in a $3 \times 3$
maze environment like the one shown in
Figure <a href="#/">maze-3x3-figure</a>. The agent knows that its
initial location is (3,3), that the goal is at (1,1), and that the four
actions <em>Up</em>, <em>Down</em>, <em>Left</em>, <em>Right</em> have their usual
effects unless blocked by a wall. The agent does <em>not</em> know
where the internal walls are. In any given state, the agent perceives
the set of legal actions; it can also tell whether the state is one it
has visited before or is a new state.</p>

<ol>
  <li>
    <p>Explain how this online search problem can be viewed as an offline
search in belief-state space, where the initial belief state
includes all possible environment configurations. How large is the
initial belief state? How large is the space of belief states?</p>
  </li>
  <li>
    <p>How many distinct percepts are possible in the initial state?</p>
  </li>
  <li>
    <p>Describe the first few branches of a contingency plan for this
problem. How large (roughly) is the complete plan?</p>
  </li>
</ol>

<p>Notice that this contingency plan is a solution for <em>every
possible environment</em> fitting the given description. Therefore,
interleaving of search and execution is not strictly necessary even in
unknown environments.</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_15" data-rating="0"></i></div>
<p><a href="ex_15/">Exercise 4.15 [path-planning-hc-exercise]</a></p>

<p>In this exercise, we examine hill climbing
in the context of robot navigation, using the environment in
Figure <a href="#/">geometric-scene-figure</a> as an example.</p>

<ol>
  <li>
    <p>Repeat Exercise <a href="#/">path-planning-agent-exercise</a> using
hill climbing. Does your agent ever get stuck in a local minimum? Is
it <em>possible</em> for it to get stuck with convex
obstacles?</p>
  </li>
  <li>
    <p>Construct a nonconvex polygonal environment in which the agent
gets stuck.</p>
  </li>
  <li>
    <p>Modify the hill-climbing algorithm so that, instead of doing a
depth-1 search to decide where to go next, it does a
depth-$k$ search. It should find the best $k$-step path and do one
step along it, and then repeat the process.</p>
  </li>
  <li>
    <p>Is there some $k$ for which the new algorithm is guaranteed to
escape from local minima?</p>
  </li>
  <li>
    <p>Explain how LRTA enables the agent to escape from local minima in
this case.</p>
  </li>
</ol>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_16" data-rating="0"></i></div>
<p><a href="ex_16/">Exercise 4.16</a></p>

<p>Like DFS, online DFS is incomplete for reversible state spaces with
infinite paths. For example, suppose that states are points on the
infinite two-dimensional grid and actions are unit vectors $(1,0)$,
$(0,1)$, $(-1,0)$, $(0,-1)$, tried in that order. Show that online DFS
starting at $(0,0)$ will not reach $(1,-1)$. Suppose the agent can
observe, in addition to its current state, all successor states and the
actions that would lead to them. Write an algorithm that is complete
even for bidirected state spaces with infinite paths. What states does
it visit in reaching $(1,-1)$?</p>

<div><i class="arrow-up loader" data-chapter="advanced-search-exercises" data-exercise="ex_17" data-rating="0"></i></div>
<p><a href="ex_17/">Exercise 4.17</a></p>

<p>Relate the time complexity of LRTA* to its space complexity.</p>


  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'nalinc';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


</article>

<script type="text/javascript">
var chapter  = String('/advanced-search-exercises/')
var chapterName = chapter.match(/\/([^\/]*)\//, "")[1]
$.get( "https://aima-exercises.firebaseapp.com/rating/"+chapterName, function( data ) {
  console.log(data)
  $("i[data-chapter='"+chapterName+"']").each(function(index,element){
  	ex = $(element).data("exercise")
  	if(ex in data){
  		console.log(data[ex])
  		$(element).attr("data-rating",data[ex])
  	}
    $(".arrow-up").removeClass("loader")
  })
});

$(document).on('click',"i[data-chapter]",function(e){
	ele = $(e.target)
  ele.addClass("loader")
	exerciseName = ele.data("exercise")
	$.post( "https://aima-exercises.firebaseapp.com/rating/"+chapterName+"/"+exerciseName, function( data ) {
	  console.log(data)
	  ele.attr("data-rating",data["rating"])
    ele.removeClass("loader")
	});	
})


</script>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
        </footer>
      </div>
    </div>

    


  </body>
</html>
