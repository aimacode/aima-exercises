<!DOCTYPE html>
<html>
  <head>
    <title>Natural Language for Communication –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Natural Language for Communication" />
    <meta property="twitter:title" content="Natural Language for Communication" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="https://code.jquery.com/jquery-3.3.1.js"></script>
    <script src="//www.gstatic.com/firebasejs/5.0.4/firebase.js"></script>
    <script type="text/javascript" src="//aima-exercises.firebaseapp.com/config.js"></script>
    <script src="//http://www.gstatic.com/firebasejs/5.0.4/firebase-firestore.js"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
<!--     <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 -->
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <input type="checkbox" id="toggleheader">
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <center>
            <h1>Aritificial Intelligence: A Modern Approach</h1>
            <h3>Stuart J. Russell and Peter Norvig</h3>
          </center>
        </header>
      </div>
    </div>
    <input type="checkbox" id="toggletoc">
    <div class="toc">
      <div>Table of Contents</div>
      <ul>
	<li>
		<span>Part &#x2160; Artificial Intelligence</span>
		<ol>
			<li><a href="/intro-exercises">1. Introduction</a></li>
			<li><a href="/agents-exercises">2. Intelligent Agent</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2161; Problem-solving</span>
		<ol>
			<li><a href="/search-exercises">3. Solving Problems By Searching</a></li>
			<li><a href="/advanced-search-exercises">4. Beyond Classical Search</a></li>
			<li><a href="/game-playing-exercises">5. Adversarial Search</a></li>
			<li><a href="/csp-exercises">6. Constraint Satisfaction Problems</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2162; Knowledge, reasoning, and planning</span>
		<ol>
			<li><a href="/knowledge-logic-exercises">7. Logical Agents</a></li>
			<li><a href="/fol-exercises">8. First Order Logic</a></li>
			<li><a href="/logical-inference-exercises">9. Inference In First Order Logic</a></li>
			<li><a href="/planning-exercises">10. Classical Planning</a></li>
			<li><a href="/advanced-planning-exercises">11. Planning And Acting In The Real World</a></li>
			<li><a href="/kr-exercises">12. Knowledge Representation</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2163; Uncertain knowledge and reasoning</span>
		<ol>
			<li><a href="/probability-exercises">13. Quantifying Uncertainity</a></li>
			<li><a href="/bayes-nets-exercises">14. Probabilistic Reasoning</a></li>
			<li><a href="/dbn-exercises">15. Probabilistic Reasoning Over Time</a></li>
			<li><a href="/decision-theory-exercises">16. Making Simple Decisions</a></li>
			<li><a href="/complex-decisions-exercises">17. Making Complex Decision</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2164; Learning</span>
		<ol>
			<li><a href="/concept-learning-exercises">18. Learning From Examples</a></li>
			<li><a href="/ilp-exercises">19. Knowledge In Learning</a></li>
			<li><a href="/bayesian-learning-exercises">20. Learning Probabilistic Models</a></li>
			<li><a href="/reinforcement-learning-exercises">21. Reinforcement Learning</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2165; Communicating, perceiving, and acting</span>
		<ol>
			<li><a href="/nlp-communicating-exercises">22. Natural Language Processing</a></li>
			<li><a href="/nlp-english-exercises">23. Natural Language For Communication</a></li>
			<li><a href="/perception-exercises">24. Perception</a></li>
			<li><a href="/robotics-exercises">25. Robotics</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2166; Conclusions</span>
		<ol>
			<li><a href="/philosophy-exercises">26. Philosophical Foundations</a></li>
			<li><a href="/#/"> Future Exercises</a></li>
		</ol>
	</li>
</ul>
    </div>
    <div id="main" role="main" class="container">
      



<ul class="breadcrumb">
<label for="toggletoc" class="toc-icon">
  <span></span>
  <span></span>
  <span></span>
</label>

   
    <li><a class="breadcrumb-text" href="/">home</a> &nbsp; </li>
   

<label for="toggleheader" class="toggleheader" title="Toggle Header">
    &#9167;
</label>
</ul>

      <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="23-natural-language-for-communication">23. Natural Language for Communication</h1>

<p><a href="23-1/">Exercise 23.1 [washing-clothes-exercise]</a></p>

<p>Read the following text once for
understanding, and remember as much of it as you can. There will be a
test later.</p>

<blockquote>
  <p>The procedure is actually quite simple. First you arrange things into
different groups. Of course, one pile may be sufficient depending on how
much there is to do. If you have to go somewhere else due to lack of
facilities that is the next step, otherwise you are pretty well set. It
is important not to overdo things. That is, it is better to do too few
things at once than too many. In the short run this may not seem
important but complications can easily arise. A mistake is expensive as
well. At first the whole procedure will seem complicated. Soon, however,
it will become just another facet of life. It is difficult to foresee
any end to the necessity for this task in the immediate future, but then
one can never tell. After the procedure is completed one arranges the
material into different groups again. Then they can be put into their
appropriate places. Eventually they will be used once more and the whole
cycle will have to be repeated. However, this is part of life.</p>
</blockquote>

<p><a href="23-2/">Exercise 23.2</a></p>

<p>An <em>HMM grammar</em> is essentially a standard HMM whose state
variable is $N$ (nonterminal, with values such as $Det$, $Adjective$,
$Noun$ and so on) and whose evidence variable is $W$ (word, with values
such as $is$, $duck$, and so on). The HMM model includes a prior
${\textbf{P}}(N_0)$, a transition model
${\textbf{P}}(N_{t+1}|N_t)$, and a sensor model
${\textbf{P}}(W_t|N_t)$. Show that every HMM grammar can be
written as a PCFG. [Hint: start by thinking about how the HMM prior can
be represented by PCFG rules for the sentence symbol. You may find it
helpful to illustrate for the particular HMM with values $A$, $B$ for
$N$ and values $x$, $y$ for $W$.]</p>

<p><a href="23-3/">Exercise 23.3</a></p>

<p>Consider the following PCFG for simple verb phrases:</p>

<blockquote>
  <p>0.1: VP $\rightarrow$ Verb</p>
</blockquote>

<blockquote>
  <p>0.2: VP $\rightarrow$ Copula Adjective</p>
</blockquote>

<blockquote>
  <p>0.5: VP $\rightarrow$ Verb the Noun</p>
</blockquote>

<blockquote>
  <p>0.2: VP $\rightarrow$ VP Adverb</p>
</blockquote>

<blockquote>
  <p>0.5: Verb $\rightarrow$ is</p>
</blockquote>

<blockquote>
  <p>0.5: Verb $\rightarrow$ shoots</p>
</blockquote>

<blockquote>
  <p>0.8: Copula $\rightarrow$ is</p>
</blockquote>

<blockquote>
  <p>0.2: Copula $\rightarrow$ seems</p>
</blockquote>

<blockquote>
  <p>0.5: Adjective $\rightarrow$ <strong>unwell</strong></p>
</blockquote>

<blockquote>
  <p>0.5: Adjective $\rightarrow$ <strong>well</strong></p>
</blockquote>

<blockquote>
  <p>0.5: Adverb $\rightarrow$ <strong>well</strong></p>
</blockquote>

<blockquote>
  <p>0.5: Adverb $\rightarrow$ <strong>badly</strong></p>
</blockquote>

<blockquote>
  <p>0.6: Noun $\rightarrow$ <strong>duck</strong></p>
</blockquote>

<blockquote>
  <p>0.4: Noun $\rightarrow$ <strong>well</strong></p>
</blockquote>

<ol>
  <li>
    <p>Which of the following have a nonzero probability as a VP? (i)
shoots the duck well well well(ii) seems the well well(iii) shoots
the unwell well badly</p>
  </li>
  <li>
    <p>What is the probability of generating “is well well”?</p>
  </li>
  <li>
    <p>What types of ambiguity are exhibited by the phrase in (b)?</p>
  </li>
  <li>
    <p>Given any PCFG, is it possible to calculate the probability that the
PCFG generates a string of exactly 10 words?</p>
  </li>
</ol>

<p><a href="23-4/">Exercise 23.4</a></p>

<p>Consider the following simple PCFG for noun phrases:</p>

<blockquote>
  <p>0.6: NP $\rightarrow$ Det\ AdjString\ Noun</p>
</blockquote>

<blockquote>
  <p>0.4: NP $\rightarrow$ Det\ NounNounCompound</p>
</blockquote>

<blockquote>
  <p>0.5: AdjString $\rightarrow$ Adj\ AdjString</p>
</blockquote>

<blockquote>
  <p>0.5: AdjString $\rightarrow$ $\Lambda$</p>
</blockquote>

<blockquote>
  <p>1.0: NounNounCompound $\rightarrow$ Noun</p>
</blockquote>

<blockquote>
  <p>0.8: Det $\rightarrow$ <strong>the</strong></p>
</blockquote>

<blockquote>
  <p>0.2: Det $\rightarrow$ <strong>a</strong></p>
</blockquote>

<blockquote>
  <p>0.5: Adj $\rightarrow$ <strong>small</strong></p>
</blockquote>

<blockquote>
  <p>0.5: Adj $\rightarrow$ <strong>green</strong></p>
</blockquote>

<blockquote>
  <p>0.6: Noun $\rightarrow$ <strong>village</strong></p>
</blockquote>

<blockquote>
  <p>0.4: Noun $\rightarrow$ <strong>green</strong></p>
</blockquote>

<p>where $\Lambda$ denotes the empty string.</p>

<ol>
  <li>
    <p>What is the longest NP that can be generated by this grammar? (i)
three words(ii) four words(iii) infinitely many words</p>
  </li>
  <li>
    <p>Which of the following have a nonzero probability of being generated
as complete NPs? (i) a small green village(ii) a green
green green(iii) a small village green</p>
  </li>
  <li>
    <p>What is the probability of generating “the green green”?</p>
  </li>
  <li>
    <p>What types of ambiguity are exhibited by the phrase in (c)?</p>
  </li>
  <li>
    <p>Given any PCFG and any finite word sequence, is it possible to
calculate the probability that the sequence was generated by the
PCFG?</p>
  </li>
</ol>

<p><a href="23-5/">Exercise 23.5</a></p>

<p>Outline the major differences between Java (or any other computer
language with which you are familiar) and English, commenting on the
“understanding” problem in each case. Think about such things as
grammar, syntax, semantics, pragmatics, compositionality,
context-dependence, lexical ambiguity, syntactic ambiguity, reference
finding (including pronouns), background knowledge, and what it means to
“understand” in the first place.</p>

<p><a href="23-6/">Exercise 23.6</a></p>

<p>This exercise concerns grammars for very simple languages.</p>

<ol>
  <li>
    <p>Write a context-free grammar for the language $a^n b^n$.</p>
  </li>
  <li>
    <p>Write a context-free grammar for the palindrome language: the set of
all strings whose second half is the reverse of the first half.</p>
  </li>
  <li>
    <p>Write a context-sensitive grammar for the duplicate language: the
set of all strings whose second half is the same as the first half.</p>
  </li>
</ol>

<p><a href="23-7/">Exercise 23.7</a></p>

<p>Consider the sentence “Someone walked slowly to the supermarket” and a
lexicon consisting of the following words:</p>

<p>$Pronoun \rightarrow \textbf{someone} \quad Verb \rightarrow \textbf{walked}$</p>

<p>$Adv \rightarrow \textbf{slowly} \quad Prep \rightarrow \textbf{to}$</p>

<p>$Article \rightarrow \textbf{the} \quad Noun \rightarrow \textbf{supermarket}$</p>

<p>Which of the following three grammars, combined with the lexicon,
generates the given sentence? Show the corresponding parse tree(s).</p>

<table>
  <thead>
    <tr>
      <th>$\quad\quad\quad\quad (A):\quad\quad\quad\quad$</th>
      <th>$\quad\quad\quad\quad(B):\quad\quad\quad\quad$</th>
      <th>$\quad\quad\quad\quad(C):\quad\quad\quad\quad$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$S\rightarrow NP\space VP$</td>
      <td>$S\rightarrow NP\space VP$</td>
      <td>$S\rightarrow NP\space VP$</td>
    </tr>
    <tr>
      <td>$NP\rightarrow Pronoun$</td>
      <td>$NP\rightarrow Pronoun$</td>
      <td>$NP\rightarrow Pronoun$</td>
    </tr>
    <tr>
      <td>$NP\rightarrow Article\space Noun $</td>
      <td>$NP\rightarrow Noun$</td>
      <td>$NP\rightarrow Article\space NP$</td>
    </tr>
    <tr>
      <td>$VP\rightarrow VP\space PP$</td>
      <td>$NP\rightarrow Article\space NP$</td>
      <td>$VP\rightarrow Verb\space Adv$</td>
    </tr>
    <tr>
      <td>$VP\rightarrow VP\space Adv\space Adv$</td>
      <td>$VP\rightarrow Verb\space Vmod$</td>
      <td>$Adv\rightarrow Adv\space Adv$</td>
    </tr>
    <tr>
      <td>$VP\rightarrow Verb$</td>
      <td>$Vmod\rightarrow Adv\space Vmod$</td>
      <td>$Adv\rightarrow PP$</td>
    </tr>
    <tr>
      <td>$PP\rightarrow Prep\space NP$</td>
      <td>$Vmod\rightarrow Adv$</td>
      <td>$PP\rightarrow Prep\space NP$</td>
    </tr>
    <tr>
      <td>$NP\rightarrow Noun$</td>
      <td>$Adv\rightarrow PP$</td>
      <td>$NP\rightarrow Noun$</td>
    </tr>
    <tr>
      <td>$\quad$</td>
      <td>$PP\rightarrow Prep\space NP$</td>
      <td>$\quad$</td>
    </tr>
  </tbody>
</table>

<p>For each of the preceding three grammars, write down three sentences of
English and three sentences of non-English generated by the grammar.
Each sentence should be significantly different, should be at least six
words long, and should include some new lexical entries (which you
should define). Suggest ways to improve each grammar to avoid generating
the non-English sentences.</p>

<p><a href="23-8/">Exercise 23.8</a></p>

<p>Collect some examples of time expressions, such as “two o’clock,”
“midnight,” and “12:46.” Also think up some examples that are
ungrammatical, such as “thirteen o’clock” or “half past two fifteen.”
Write a grammar for the time language.</p>

<p><a href="23-9/">Exercise 23.9</a></p>

<p>Some linguists have argued as follows:</p>

<blockquote>
  <p>Children learning a language hear only <em>positive
examples</em> of the language and no <em>negative
examples</em>. Therefore, the hypothesis that “every possible
sentence is in the language” is consistent with all the observed
examples. Moreover, this is the simplest consistent hypothesis.
Furthermore, all grammars for languages that are supersets of the true
language are also consistent with the observed data. Yet children do
induce (more or less) the right grammar. It follows that they begin
with very strong innate grammatical constraints that rule out all of
these more general hypotheses <em>a priori</em>.</p>
</blockquote>

<p>Comment on the weak point(s) in this argument from a statistical
learning viewpoint.</p>

<p><a href="23-10/">Exercise 23.10 [chomsky-form-exercise]</a></p>

<p>In this exercise you will transform $\large \varepsilon_0$  into
Chomsky Normal Form (CNF). There are five steps: (a) Add a new start
symbol, (b) Eliminate $\epsilon$ rules, (c) Eliminate multiple words on
right-hand sides, (d) Eliminate rules of the form
(${\it X}$
$\rightarrow$${\it Y}$),
(e) Convert long right-hand sides into binary rules.</p>

<ol>
  <li>
    <p>The start symbol, $S$, can occur only on the left-hand side in CNF.
Replace ${\it S}$ everywhere by a new symbol
${\it S’}$ and add a rule of the form
${\it S}$
$\rightarrow$${\it S’}$.</p>
  </li>
  <li>
    <p>The empty string, $\epsilon$ cannot appear on the right-hand side
in CNF. $\large \varepsilon_0$ does not have any rules with $\epsilon$, so this is not
an issue.</p>
  </li>
  <li>
    <p>A word can appear on the right-hand side in a rule only of the form
(${\it X}$
$\rightarrow$<em>word</em>).
Replace each rule of the form (${\it X}$
$\rightarrow$…<em>word</em> …)
with (${\it X}$
$\rightarrow$…${\it W’}$ …)
and (${\it W’}$
$\rightarrow$<em>word</em>),
using a new symbol ${\it W’}$.</p>
  </li>
  <li>
    <p>A rule (${\it X}$
$\rightarrow<script type="math/tex">{\it Y}$)
is not allowed in CNF; it must be (${\it X}$
$\rightarrow</script>{\it Y}$
${\it Z}$) or (${\it X}$
$\rightarrow$<em>word</em>).
Replace each rule of the form (${\it X}$
$\rightarrow$${\it Y}$)
with a set of rules of the form (${\it X}$
$\rightarrow$…), one
for each rule (${\it Y}$
$\rightarrow$…),
where (…) indicates one or more symbols.</p>
  </li>
  <li>
    <p>Replace each rule of the form (${\it X}$
$\rightarrow<script type="math/tex">{\it Y}$
${\it Z}$ …) with two rules, (${\it X}$
$\rightarrow</script>{\it Y}$
${\it Z’}$) and (${\it Z’}$
$\rightarrow$${\it Z}$
…), where ${\it Z’}$ is a new symbol.</p>
  </li>
</ol>

<p>Show each step of the process and the final set of rules.</p>

<p><a href="23-11/">Exercise 23.11</a></p>

<p>Consider the following toy grammar:</p>

<blockquote>
  <p>$S \rightarrow NP\space VP$</p>
</blockquote>

<blockquote>
  <p>$NP \rightarrow Noun$</p>
</blockquote>

<blockquote>
  <p>$NP \rightarrow NP\space and\space NP$</p>
</blockquote>

<blockquote>
  <p>$NP \rightarrow NP\space PP$</p>
</blockquote>

<blockquote>
  <p>$VP \rightarrow Verb$</p>
</blockquote>

<blockquote>
  <p>$VP \rightarrow VP\space and \space VP$</p>
</blockquote>

<blockquote>
  <p>$VP \rightarrow VP\space PP$</p>
</blockquote>

<blockquote>
  <p>$PP \rightarrow Prep\space NP$</p>
</blockquote>

<blockquote>
  <p>$Noun \rightarrow Sally\space; pools\space; streams\space; swims$</p>
</blockquote>

<blockquote>
  <p>$Prep \rightarrow in$</p>
</blockquote>

<blockquote>
  <p>$Verb \rightarrow pools\space; streams\space; swims$</p>
</blockquote>

<ol>
  <li>
    <p>Show all the parse trees in this grammar for the sentence “Sally
swims in streams and pools.”</p>
  </li>
  <li>
    <p>Show all the table entries that would be made by
a (non-probabalistic) CYK parser on this sentence.</p>
  </li>
</ol>

<p><a href="23-12/">Exercise 23.12 [exercise-subj-verb-agree]</a></p>

<p>Using DCG notation, write a grammar for a
language that is just like $\large \varepsilon_1$, except that it enforces agreement between
the subject and verb of a sentence and thus does not generate
ungrammatical sentences such as “I smells the wumpus.”</p>

<p><a href="23-13/">Exercise 23.13</a></p>

<p>Consider the following PCFG:</p>

<blockquote>
  <p>$S \rightarrow NP \space VP[1.0] $</p>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>$NP \rightarrow \textit{Noun}[0.6] \space</td>
        <td>\space \textit{Pronoun}[0.4] $</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>$VP \rightarrow \textit{Verb} \space NP[0.8] \space</td>
        <td>\space \textit{Modal}\space \textit{Verb}[0.2]$</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>$\textit{Noun} \rightarrow \textbf{can}[0.1] \space</td>
        <td>\space \textbf{fish}[0.3] \space</td>
        <td>\space …$</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>$\textit{Pronoun} \rightarrow \textbf{I}[0.4] \space</td>
        <td>\space …$</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>$\textit{Verb} \rightarrow \textbf{can}[0.01] \space</td>
        <td>\space \textbf{fish}[0.1] \space</td>
        <td>\space …$</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<blockquote>
  <table>
    <tbody>
      <tr>
        <td>$\textit{Modal} \rightarrow \textbf{can}[0.3] \space</td>
        <td>\space …$</td>
      </tr>
    </tbody>
  </table>
</blockquote>

<p>The sentence “I can fish” has two parse trees with this grammar. Show
the two trees, their prior probabilities, and their conditional
probabilities, given the sentence.</p>

<p><a href="23-14/">Exercise 23.14</a></p>

<p>An augmented context-free grammar can represent languages that a regular
context-free grammar cannot. Show an augmented context-free grammar for
the language $a^nb^nc^n$. The allowable values for augmentation
variables are 1 and $SUCCESSOR(n)$, where $n$ is a value. The rule for a sentence
in this language is
<script type="math/tex">S(n) \rightarrow}}A(n) B(n) C(n) \ .</script>
Show the rule(s) for each of ${\it A}$,
${\it B}$, and ${\it C}$.</p>

<p><a href="23-15/">Exercise 23.15</a></p>

<p>Augment the $\large \varepsilon_1$ grammar so that it handles article–noun agreement. That is,
make sure that “agents” and “an agent” are ${\it NP}$s, but
“agent” and “an agents” are not.</p>

<p><a href="23-16/">Exercise 23.16</a></p>

<p>Consider the following sentence (from <em>The New York Times,</em>
July 28, 2008):</p>

<blockquote>
  <p>Banks struggling to recover from multibillion-dollar loans on real
estate are curtailing loans to American businesses, depriving even
healthy companies of money for expansion and hiring.</p>
</blockquote>

<ol>
  <li>
    <p>Which of the words in this sentence are lexically ambiguous?</p>
  </li>
  <li>
    <p>Find two cases of syntactic ambiguity in this sentence (there are
more than two.)</p>
  </li>
  <li>
    <p>Give an instance of metaphor in this sentence.</p>
  </li>
  <li>
    <p>Can you find semantic ambiguity?</p>
  </li>
</ol>

<p><a href="23-17/">Exercise 23.17 [washing-clothes2-exercise]</a></p>

<p>Without looking back at
Exercise <a href="#/">washing-clothes-exercise</a>, answer the following
questions:</p>

<ol>
  <li>
    <p>What are the four steps that are mentioned?</p>
  </li>
  <li>
    <p>What step is left out?</p>
  </li>
  <li>
    <p>What is “the material” that is mentioned in the text?</p>
  </li>
  <li>
    <p>What kind of mistake would be expensive?</p>
  </li>
  <li>
    <p>Is it better to do too few things or too many? Why?</p>
  </li>
</ol>

<p><a href="23-18/">Exercise 23.18</a></p>

<p>Select five sentences and submit them to an online translation service.
Translate them from English to another language and back to English.
Rate the resulting sentences for grammaticality and preservation of
meaning. Repeat the process; does the second round of iteration give
worse results or the same results? Does the choice of intermediate
language make a difference to the quality of the results? If you know a
foreign language, look at the translation of one paragraph into that
language. Count and describe the errors made, and conjecture why these
errors were made.</p>

<p><a href="23-19/">Exercise 23.19</a></p>

<p>The $D_i$ values for the sentence in
Figure <a href="#/">mt-alignment-figure</a> sum to 0. Will that be true
of every translation pair? Prove it or give a counterexample.</p>

<p><a href="23-20/">Exercise 23.20</a></p>

<p>(Adapted from [@Knight:1999].) Our translation model assumes that, after the phrase
translation model selects phrases and the distortion model permutes
them, the language model can unscramble the permutation. This exercise
investigates how sensible that assumption is. Try to unscramble these
proposed lists of phrases into the correct order:</p>

<ol>
  <li>
    <p>have, programming, a, seen, never, I, language, better</p>
  </li>
  <li>
    <p>loves, john, mary</p>
  </li>
  <li>
    <p>is the, communication, exchange of, intentional, information
brought, by, about, the production, perception of, and signs, from,
drawn, a, of, system, signs, conventional, shared</p>
  </li>
  <li>
    <p>created, that, we hold these, to be, all men, truths, are, equal,
self-evident</p>
  </li>
</ol>

<p>Which ones could you do? What type of knowledge did you draw upon? Train
a bigram model from a training corpus, and use it to find the
highest-probability permutation of some sentences from a test corpus.
Report on the accuracy of this model.</p>

<p><a href="23-21/">Exercise 23.21</a></p>

<p>Calculate the most probable path through the HMM in
Figure <a href="#/">sr-hmm-figure</a> for the output sequence
$[C_1,C_2,C_3,C_4,C_4,C_6,C_7]$. Also give its probability.</p>

<p><a href="23-22/">Exercise 23.22</a></p>

<p>We forgot to mention that the text in
Exercise <a href="#/">washing-clothes-exercise</a> is entitled “Washing
Clothes.” Reread the text and answer the questions in
Exercise <a href="#/">washing-clothes2-exercise</a>. Did you do better
this time? Bransford and Johnson [@Bransford+Johnson:1973] used this
text in a controlled experiment and found that the title helped
significantly. What does this tell you about how language and memory
works?</p>


  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!--            Designed by <a href="http://nalinc.github.io" style="color:#2196F3">Nalin</a> &#9889; 
           Written in <a href="#/" style="color: #2a932a">Markdown</a> &#9889; 
           Powered by <a href="http://jekyllrb.com" style="color: #d73838">Jekyll</a>    -->
          <!-- 











 -->
        </footer>
      </div>
    </div>

    


    <script type="text/javascript">
      firestore =firebase.firestore();
      function rateExercise(e){
        console.log(e.target)
        chapterLabel = $(e.target).data("chapter")
        exerciseLabel = $(e.target).data("exercise")
        docRef = firestore.collection("rating").doc(chapterLabel)
        score = 0
        docRef.get().then(function(doc){
          if (doc && doc.exists){
            myData = doc.data()
            // score = myData
            console.log(myData)
            if(exerciseLabel in myData){
              myData[exerciseLabel] += 1
            }else{
              myData[exerciseLabel] = 1
            }
            // $(e.target).data("rating",score);
            docRef.set(myData).then(function(){
              console.log("status saved")
              console.log(myData[exerciseLabel])
              $(e.target).attr("data-rating",myData[exerciseLabel]);
            })
          }
          // else{
          //   myData = {
          //     "ex_1":0,
          //     "ex_2":0,
          //     "ex_3":0,
          //     "ex_4":0,
          //     "ex_5":0,
          //     "ex_6":0,
          //     "ex_7":0,
          //     "ex_8":0,
          //     "ex_9":0,
          //     "ex_10":0
          //   }
          //   docRef.set(myData).then(function(){
          //     console.log("status saved")
          //     console.log(myData[exerciseLabel])
          //     // $(e.target).attr("data-rating",myData[exerciseLabel]);
          //   })
          // }
        })
      }
      getRealTimeUpdates = function(){
        docRef = firestore.collection("rating").doc("intro-exercises");
        docRef.onSnapshot(function(doc){
          if (doc && doc.exists){
            myData = doc.data()
            for(key in myData){
              console.log(key)  
              // $(e.target).attr("data-rating",myData[exerciseLabel]);
            }
          }
        })
      }

      getRealTimeUpdates()
      $(document).on("click",".arrow-up", rateExercise)
    </script>
  </body>
</html>
