<!DOCTYPE html>
<html>
  <head>
    <title>Reinforcement Learning –  – </title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="" />

    
    <meta property="og:title" content="Reinforcement Learning" />
    <meta property="twitter:title" content="Reinforcement Learning" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script src="https://code.jquery.com/jquery-3.3.1.js"></script>
    <script src="//www.gstatic.com/firebasejs/5.0.4/firebase.js"></script>
    <script type="text/javascript" src="//aima-exercises.firebaseapp.com/config.js"></script>
    <script src="//http://www.gstatic.com/firebasejs/5.0.4/firebase-firestore.js"></script>

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title=" - " href="/feed.xml" />
<!--     <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
 -->
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <input type="checkbox" id="toggleheader">
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <center>
            <h1>Aritificial Intelligence: A Modern Approach</h1>
            <h3>Stuart J. Russell and Peter Norvig</h3>
          </center>
        </header>
      </div>
    </div>
    <input type="checkbox" id="toggletoc">
    <div class="toc">
      <div>Table of Contents</div>
      <ul>
	<li>
		<span>Part &#x2160; Artificial Intelligence</span>
		<ol>
			<li><a href="/intro-exercises">1. Introduction</a></li>
			<li><a href="/agents-exercises">2. Intelligent Agent</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2161; Problem-solving</span>
		<ol>
			<li><a href="/search-exercises">3. Solving Problems By Searching</a></li>
			<li><a href="/advanced-search-exercises">4. Beyond Classical Search</a></li>
			<li><a href="/game-playing-exercises">5. Adversarial Search</a></li>
			<li><a href="/csp-exercises">6. Constraint Satisfaction Problems</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2162; Knowledge, reasoning, and planning</span>
		<ol>
			<li><a href="/knowledge-logic-exercises">7. Logical Agents</a></li>
			<li><a href="/fol-exercises">8. First Order Logic</a></li>
			<li><a href="/logical-inference-exercises">9. Inference In First Order Logic</a></li>
			<li><a href="/planning-exercises">10. Classical Planning</a></li>
			<li><a href="/advanced-planning-exercises">11. Planning And Acting In The Real World</a></li>
			<li><a href="/kr-exercises">12. Knowledge Representation</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2163; Uncertain knowledge and reasoning</span>
		<ol>
			<li><a href="/probability-exercises">13. Quantifying Uncertainity</a></li>
			<li><a href="/bayes-nets-exercises">14. Probabilistic Reasoning</a></li>
			<li><a href="/dbn-exercises">15. Probabilistic Reasoning Over Time</a></li>
			<li><a href="/decision-theory-exercises">16. Making Simple Decisions</a></li>
			<li><a href="/complex-decisions-exercises">17. Making Complex Decision</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2164; Learning</span>
		<ol>
			<li><a href="/concept-learning-exercises">18. Learning From Examples</a></li>
			<li><a href="/ilp-exercises">19. Knowledge In Learning</a></li>
			<li><a href="/bayesian-learning-exercises">20. Learning Probabilistic Models</a></li>
			<li><a href="/reinforcement-learning-exercises">21. Reinforcement Learning</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2165; Communicating, perceiving, and acting</span>
		<ol>
			<li><a href="/nlp-communicating-exercises">22. Natural Language Processing</a></li>
			<li><a href="/nlp-english-exercises">23. Natural Language For Communication</a></li>
			<li><a href="/perception-exercises">24. Perception</a></li>
			<li><a href="/robotics-exercises">25. Robotics</a></li>
		</ol>
	</li>
	<li>
		<span>Part &#x2166; Conclusions</span>
		<ol>
			<li><a href="/philosophy-exercises">26. Philosophical Foundations</a></li>
			<li><a href="/#/"> Future Exercises</a></li>
		</ol>
	</li>
</ul>
    </div>
    <div id="main" role="main" class="container">
      



<ul class="breadcrumb">
<label for="toggletoc" class="toc-icon">
  <span></span>
  <span></span>
  <span></span>
</label>

   
    <li><a class="breadcrumb-text" href="/">home</a> &nbsp; </li>
   

<label for="toggleheader" class="toggleheader" title="Toggle Header">
    &#9167;
</label>
</ul>

      <article class="post">

  <div class="entry">
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    },
    "HTML-CSS": { 
      preferredFont: "TeX", 
      availableFonts: ["STIX","TeX"], 
      styles: {".MathJax": {}} 
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<h1 id="21-reinforcement-learning">21. Reinforcement Learning</h1>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_1" data-rating="0"></i></div>
<p><a href="21-1/">Exercise 21.1</a></p>

<p>Implement a passive learning agent in a simple environment, such as the
$4\times 3$ world. For the case of an initially unknown environment
model, compare the learning performance of the direct utility
estimation, TD, and ADP algorithms. Do the comparison for the optimal
policy and for several random policies. For which do the utility
estimates converge faster? What happens when the size of the environment
is increased? (Try environments with and without obstacles.)</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_2" data-rating="0"></i></div>
<p><a href="21-2/">Exercise 21.2</a></p>

<p>Chapter <a href="#/">complex-decisions-chapter</a> defined a
<strong>proper policy</strong> for an MDP as one that is
guaranteed to reach a terminal state. Show that it is possible for a
passive ADP agent to learn a transition model for which its policy $\pi$
is improper even if $\pi$ is proper for the true MDP; with such models,
the POLICY-EVALUATION step may fail if $\gamma1$. Show that this problem cannot
arise if POLICY-EVALUATION is applied to the learned model only at the end of a trial.</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_3" data-rating="0"></i></div>
<p><a href="21-3/">Exercise 21.3 [prioritized-sweeping-exercise]</a></p>

<p>Starting with the passive ADP agent,
modify it to use an approximate ADP algorithm as discussed in the text.
Do this in two steps:</p>

<ol>
  <li>
    <p>Implement a priority queue for adjustments to the utility estimates.
Whenever a state is adjusted, all of its predecessors also become
candidates for adjustment and should be added to the queue. The
queue is initialized with the state from which the most recent
transition took place. Allow only a fixed number of adjustments.</p>
  </li>
  <li>
    <p>Experiment with various heuristics for ordering the priority queue,
examining their effect on learning rates and computation time.</p>
  </li>
</ol>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_4" data-rating="0"></i></div>
<p><a href="21-4/">Exercise 21.4</a></p>

<p>The direct utility estimation method in
Section <a href="#/">passive-rl-section</a> uses distinguished terminal
states to indicate the end of a trial. How could it be modified for
environments with discounted rewards and no terminal states?</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_5" data-rating="0"></i></div>
<p><a href="21-5/">Exercise 21.5</a></p>

<p>Write out the parameter update equations for TD learning with
<script type="math/tex">\hat{U}(x,y) = \theta_0 + \theta_1 x + \theta_2 y + \theta_3\,\sqrt{(x-x_g)^2 + (y-y_g)^2}\ .</script></p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_6" data-rating="0"></i></div>
<p><a href="21-6/">Exercise 21.6</a></p>

<p>Adapt the vacuum world (Chapter <a href="#/">agents-chapter</a>) for
reinforcement learning by including rewards for squares being clean.
Make the world observable by providing suitable percepts. Now experiment
with different reinforcement learning agents. Is function approximation
necessary for success? What sort of approximator works for this
application?</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_7" data-rating="0"></i></div>
<p><a href="21-7/">Exercise 21.7 [approx-LMS-exercise]</a></p>

<p>Implement an exploring reinforcement learning
agent that uses direct utility estimation. Make two versions—one with a
tabular representation and one using the function approximator in
Equation (<a href="#/">4x3-linear-approx-equation</a>). Compare their
performance in three environments:</p>

<ol>
  <li>
    <p>The $4\times 3$ world described in the chapter.</p>
  </li>
  <li>
    <p>A ${10}\times {10}$ world with no obstacles and a +1 reward
at (10,10).</p>
  </li>
  <li>
    <p>A ${10}\times {10}$ world with no obstacles and a +1 reward
at (5,5).</p>
  </li>
</ol>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_8" data-rating="0"></i></div>
<p><a href="21-8/">Exercise 21.8</a></p>

<p>Devise suitable features for reinforcement learning in stochastic grid
worlds (generalizations of the $4\times 3$ world) that contain multiple
obstacles and multiple terminal states with rewards of $+1$ or $-1$.</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_9" data-rating="0"></i></div>
<p><a href="21-9/">Exercise 21.9</a></p>

<p>Extend the standard game-playing environment
(Chapter <a href="#/">game-playing-chapter</a>) to incorporate a reward
signal. Put two reinforcement learning agents into the environment (they
may, of course, share the agent program) and have them play against each
other. Apply the generalized TD update rule
(Equation (<a href="#/">generalized-td-equation</a>)) to update the
evaluation function. You might wish to start with a simple linear
weighted evaluation function and a simple game, such as tic-tac-toe.</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_10" data-rating="0"></i></div>
<p><a href="21-10/">Exercise 21.10 [10x10-exercise]</a></p>

<p>Compute the true utility function and the best linear
approximation in $x$ and $y$ (as in
Equation (<a href="#/">4x3-linear-approx-equation</a>)) for the
following environments:</p>

<ol>
  <li>
    <p>A ${10}\times {10}$ world with a single $+1$ terminal state
at (10,10).</p>
  </li>
  <li>
    <p>As in (a), but add a $-1$ terminal state at (10,1).</p>
  </li>
  <li>
    <p>As in (b), but add obstacles in 10 randomly selected squares.</p>
  </li>
  <li>
    <p>As in (b), but place a wall stretching from (5,2) to (5,9).</p>
  </li>
  <li>
    <p>As in (a), but with the terminal state at (5,5).</p>
  </li>
</ol>

<p>The actions are deterministic moves in the four directions. In each
case, compare the results using three-dimensional plots. For each
environment, propose additional features (besides $x$ and $y$) that
would improve the approximation and show the results.</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_11" data-rating="0"></i></div>
<p><a href="21-11/">Exercise 21.11</a></p>

<p>Implement the REINFORCE and PEGASUS algorithms and apply them to the $4\times 3$ world,
using a policy family of your own choosing. Comment on the results.</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_12" data-rating="0"></i></div>
<p><a href="21-12/">Exercise 21.12</a></p>

<p>Investigate the application of reinforcement learning ideas to the
modeling of human and animal behavior.</p>

<div><i class="arrow-up" data-chapter="reinforcement-learning-exercises" data-exercise="ex_13" data-rating="0"></i></div>
<p><a href="21-13/">Exercise 21.13</a></p>

<p>Is reinforcement learning an appropriate abstract model for evolution?
What connection exists, if any, between hardwired reward signals and
evolutionary fitness?</p>


  </div>

<!--   <div class="date">
    Written on 
  </div>
 -->
  

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
<!--            Designed by <a href="http://nalinc.github.io" style="color:#2196F3">Nalin</a> &#9889; 
           Written in <a href="#/" style="color: #2a932a">Markdown</a> &#9889; 
           Powered by <a href="http://jekyllrb.com" style="color: #d73838">Jekyll</a>    -->
          <!-- 











 -->
        </footer>
      </div>
    </div>

    


    <script type="text/javascript">
      firestore =firebase.firestore();
      function rateExercise(e){
        console.log(e.target)
        chapterLabel = $(e.target).data("chapter")
        exerciseLabel = $(e.target).data("exercise")
        docRef = firestore.collection("rating").doc(chapterLabel)
        score = 0
        docRef.get().then(function(doc){
          if (doc && doc.exists){
            myData = doc.data()
            // score = myData
            console.log(myData)
            if(exerciseLabel in myData){
              myData[exerciseLabel] += 1
            }else{
              myData[exerciseLabel] = 1
            }
            // $(e.target).data("rating",score);
            docRef.set(myData).then(function(){
              console.log("status saved")
              console.log(myData[exerciseLabel])
              $(e.target).attr("data-rating",myData[exerciseLabel]);
            })
          }
          // else{
          //   myData = {
          //     "ex_1":0,
          //     "ex_2":0,
          //     "ex_3":0,
          //     "ex_4":0,
          //     "ex_5":0,
          //     "ex_6":0,
          //     "ex_7":0,
          //     "ex_8":0,
          //     "ex_9":0,
          //     "ex_10":0
          //   }
          //   docRef.set(myData).then(function(){
          //     console.log("status saved")
          //     console.log(myData[exerciseLabel])
          //     // $(e.target).attr("data-rating",myData[exerciseLabel]);
          //   })
          // }
        })
      }
      getRealTimeUpdates = function(){
        docRef = firestore.collection("rating").doc("intro-exercises");
        docRef.onSnapshot(function(doc){
          if (doc && doc.exists){
            myData = doc.data()
            for(key in myData){
              console.log(key)  
              // $(e.target).attr("data-rating",myData[exerciseLabel]);
            }
          }
        })
      }

      getRealTimeUpdates()
      $(document).on("click",".arrow-up", rateExercise)
    </script>
  </body>
</html>
