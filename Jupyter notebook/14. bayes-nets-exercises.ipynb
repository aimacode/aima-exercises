{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Probabilistic Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14.1** We have a bag of three biased coins $a$, $b$, and $c$ with probabilities\n",
    "of coming up heads of 20%, 60%, and 80%, respectively. One coin is drawn\n",
    "randomly from the bag (with equal likelihood of drawing each of the\n",
    "three coins), and then the coin is flipped three times to generate the\n",
    "outcomes $X_1$, $X_2$, and $X_3$.\n",
    "\n",
    "1.  Draw the Bayesian network corresponding to this setup and define the\n",
    "    necessary CPTs.\n",
    "\n",
    "2.  Calculate which coin was most likely to have been drawn from the bag\n",
    "    if the observed flips come out heads twice and tails once.\n",
    "\n",
    "**14.2** We have a bag of three biased coins $a$, $b$, and $c$ with probabilities\n",
    "of coming up heads of 30%, 60%, and 75%, respectively. One coin is drawn\n",
    "randomly from the bag (with equal likelihood of drawing each of the\n",
    "three coins), and then the coin is flipped three times to generate the\n",
    "outcomes $X_1$, $X_2$, and $X_3$.\n",
    "\n",
    "1.  Draw the Bayesian network corresponding to this setup and define the\n",
    "    necessary CPTs.\n",
    "\n",
    "2.  Calculate which coin was most likely to have been drawn from the bag\n",
    "    if the observed flips come out heads twice and tails once.\n",
    "\n",
    "**14.3** \\[cpt-equivalence-exercise\\]\n",
    "Equation ([parameter-joint-repn-equation](#/)) on\n",
    "page [parameter-joint-repn-equation](#/) defines the joint distribution represented by a\n",
    "Bayesian network in terms of the parameters\n",
    "$\\theta(X_i{{\\,|\\,}}{Parents}(X_i))$. This exercise asks you to derive\n",
    "the equivalence between the parameters and the conditional probabilities\n",
    "${\\textbf{ P}}(X_i{{\\,|\\,}}{Parents}(X_i))$ from this definition.\n",
    "\n",
    "1.  Consider a simple network $X\\rightarrow Y\\rightarrow Z$ with three\n",
    "    Boolean variables. Use\n",
    "    Equations ([conditional-probability-equation](#/)) and ([marginalization-equation](#/))\n",
    "    (pages [conditional-probability-equation](#/) and [marginalization-equation](#/)) \n",
    "    to express the conditional probability $P(z{{\\,|\\,}}y)$ as the ratio of two sums, each over entries in the\n",
    "    joint distribution ${\\textbf{P}}(X,Y,Z)$.\n",
    "\n",
    "2.  Now use Equation ([parameter-joint-repn-equation](#/)) to\n",
    "    write this expression in terms of the network parameters\n",
    "    $\\theta(X)$, $\\theta(Y{{\\,|\\,}}X)$, and $\\theta(Z{{\\,|\\,}}Y)$.\n",
    "\n",
    "3.  Next, expand out the summations in your expression from part (b),\n",
    "    writing out explicitly the terms for the true and false values of\n",
    "    each summed variable. Assuming that all network parameters satisfy\n",
    "    the constraint\n",
    "    $\\sum_{x_i} \\theta(x_i{{\\,|\\,}}{parents}(X_i)){{\\,{=}\\,}}1$, show\n",
    "    that the resulting expression reduces to $\\theta(z{{\\,|\\,}}y)$.\n",
    "\n",
    "4.  Generalize this derivation to show that\n",
    "    $\\theta(X_i{{\\,|\\,}}{Parents}(X_i)) = {\\textbf{P}}(X_i{{\\,|\\,}}{Parents}(X_i))$\n",
    "    for any Bayesian network.\n",
    "\n",
    "**14.4** The **arc reversal** operation of in a Bayesian network allows us to change the direction\n",
    "of an arc $X\\rightarrow Y$ while preserving the joint probability\n",
    "distribution that the network represents @Shachter:1986. Arc reversal\n",
    "may require introducing new arcs: all the parents of $X$ also become\n",
    "parents of $Y$, and all parents of $Y$ also become parents of $X$.\n",
    "\n",
    "1.  Assume that $X$ and $Y$ start with $m$ and $n$ parents,\n",
    "    respectively, and that all variables have $k$ values. By calculating\n",
    "    the change in size for the CPTs of $X$ and $Y$, show that the total\n",
    "    number of parameters in the network cannot decrease during\n",
    "    arc reversal. (*Hint*: the parents of $X$ and $Y$ need\n",
    "    not be disjoint.)\n",
    "\n",
    "2.  Under what circumstances can the total number remain constant?\n",
    "\n",
    "3.  Let the parents of $X$ be $\\textbf{U} \\cup \\textbf{V}$ and the parents of $Y$ be\n",
    "    $\\textbf{V} \\cup \\textbf{W}$, where $\\textbf{U}$ and $\\textbf{W}$ are disjoint. The formulas for the\n",
    "    new CPTs after arc reversal are as follows: $$\\begin{aligned}\n",
    "    {\\textbf{P}}(Y{{\\,|\\,}}\\textbf{U},\\textbf{V},\\textbf{W}) &=& \\sum_x {\\textbf{P}}(Y{{\\,|\\,}}\\textbf{V},\\textbf{W}, x) {\\textbf{P}}(x{{\\,|\\,}}\\textbf{U}, \\textbf{V}) \\\\\n",
    "    {\\textbf{P}}(X{{\\,|\\,}}\\textbf{U},\\textbf{V},\\textbf{W}, Y) &=& {\\textbf{P}}(Y{{\\,|\\,}}X, \\textbf{V}, \\textbf{W}) {\\textbf{P}}(X{{\\,|\\,}}\\textbf{U}, \\textbf{V}) / {\\textbf{P}}(Y{{\\,|\\,}}\\textbf{U},\\textbf{V},\\textbf{W})\\ .\\end{aligned}$$\n",
    "    Prove that the new network expresses the same joint distribution\n",
    "    over all variables as the original network.\n",
    "\n",
    "**14.5** Consider the Bayesian network in\n",
    "Figure [burglary-figure](#/).\n",
    "\n",
    "1.  If no evidence is observed, are ${Burglary}$ and ${Earthquake}$\n",
    "    independent? Prove this from the numerical semantics and from the\n",
    "    topological semantics.\n",
    "\n",
    "2.  If we observe ${Alarm}{{\\,{=}\\,}}{true}$, are ${Burglary}$ and\n",
    "    ${Earthquake}$ independent? Justify your answer by calculating\n",
    "    whether the probabilities involved satisfy the definition of\n",
    "    conditional independence.\n",
    "\n",
    "**14.6** Suppose that in a Bayesian network containing an unobserved variable\n",
    "$Y$, all the variables in the Markov blanket ${MB}(Y)$ have been\n",
    "observed.\n",
    "\n",
    "1.  Prove that removing the node $Y$ from the network will not affect\n",
    "    the posterior distribution for any other unobserved variable in\n",
    "    the network.\n",
    "\n",
    "2.  Discuss whether we can remove $Y$ if we are planning to use (i)\n",
    "    rejection sampling and (ii) likelihood weighting.\n",
    "\n",
    "<center>\n",
    "<b id=\"handedness-figure\">Figure [handedness-figure]</b> Three possible structures for a Bayesian network describing genetic inheritance of handedness.\n",
    "</center>\n",
    "\n",
    "![handedness-figure](http://nalinc.github.io/aima-exercises/Jupyter%20notebook/figures/handedness1.svg)\n",
    "    \n",
    "**14.7** \\[handedness-exercise\\] Let $H_x$ be a random variable denoting the\n",
    "handedness of an individual $x$, with possible values $l$ or $r$. A\n",
    "common hypothesis is that left- or right-handedness is inherited by a\n",
    "simple mechanism; that is, perhaps there is a gene $G_x$, also with\n",
    "values $l$ or $r$, and perhaps actual handedness turns out mostly the\n",
    "same (with some probability $s$) as the gene an individual possesses.\n",
    "Furthermore, perhaps the gene itself is equally likely to be inherited\n",
    "from either of an individual’s parents, with a small nonzero probability\n",
    "$m$ of a random mutation flipping the handedness.\n",
    "\n",
    "1.  Which of the three networks in\n",
    "    Figure [handedness-figure](#handedness-figure) claim that\n",
    "    $ {\\textbf{P}}(G_{{father}},G_{{mother}},G_{{child}}) = {\\textbf{P}}(G_{{father}}){\\textbf{P}}(G_{{mother}}){\\textbf{P}}(G_{{child}})$?\n",
    "\n",
    "2.  Which of the three networks make independence claims that are\n",
    "    consistent with the hypothesis about the inheritance of handedness?\n",
    "\n",
    "3.  Which of the three networks is the best description of the\n",
    "    hypothesis?\n",
    "\n",
    "4.  Write down the CPT for the $G_{{child}}$ node in network (a), in\n",
    "    terms of $s$ and $m$.\n",
    "\n",
    "5.  Suppose that\n",
    "    $P(G_{{father}}{{\\,{=}\\,}}l)=P(G_{{mother}}{{\\,{=}\\,}}l)=q$. In\n",
    "    network (a), derive an expression for $P(G_{{child}}{{\\,{=}\\,}}l)$\n",
    "    in terms of $m$ and $q$ only, by conditioning on its parent nodes.\n",
    "\n",
    "6.  Under conditions of genetic equilibrium, we expect the distribution\n",
    "    of genes to be the same across generations. Use this to calculate\n",
    "    the value of $q$, and, given what you know about handedness in\n",
    "    humans, explain why the hypothesis described at the beginning of\n",
    "    this question must be wrong.\n",
    "\n",
    "**14.8** \\[markov-blanket-exercise\\] The **Markov\n",
    "blanket** of a variable is defined on page [markov-blanket-page](#/).\n",
    "Prove that a variable is independent of all other variables in the\n",
    "network, given its Markov blanket and derive\n",
    "Equation ([markov-blanket-equation](#/))\n",
    "(page [markov-blanket-equation](#/)).\n",
    "\n",
    "<center>\n",
    "<b id=\"car-starts-figure\">Figure [car-starts-figure]</b> A Bayesian network describing some features of a car's electrical system and engine. Each variable is Boolean, and the *true* value indicates that the corresponding aspect of the vehicle is in working order.\n",
    "</center>\n",
    "\n",
    "![car-starts-figure](http://nalinc.github.io/aima-exercises/Jupyter%20notebook/figures/car-starts.svg)\n",
    "\n",
    "**14.9** Consider the network for car diagnosis shown in\n",
    "Figure [car-starts-figure](#car-starts-figure).\n",
    "\n",
    "1.  Extend the network with the Boolean variables ${IcyWeather}$ and\n",
    "    ${StarterMotor}$.\n",
    "\n",
    "2.  Give reasonable conditional probability tables for all the nodes.\n",
    "\n",
    "3.  How many independent values are contained in the joint probability\n",
    "    distribution for eight Boolean nodes, assuming that no conditional\n",
    "    independence relations are known to hold among them?\n",
    "\n",
    "4.  How many independent probability values do your network tables\n",
    "    contain?\n",
    "\n",
    "5.  The conditional distribution for ${Starts}$ could be described as\n",
    "    a **noisy-AND** distribution. Define this\n",
    "    family in general and relate it to the noisy-OR distribution.\n",
    "\n",
    "**14.10** Consider a simple Bayesian network with root variables ${Cold}$,\n",
    "${Flu}$, and ${Malaria}$ and child variable ${Fever}$, with a\n",
    "noisy-OR conditional distribution for ${Fever}$ as described in\n",
    "Section [canonical-distribution-section](#/). By adding\n",
    "appropriate auxiliary variables for inhibition events and fever-inducing\n",
    "events, construct an equivalent Bayesian network whose CPTs (except for\n",
    "root variables) are deterministic. Define the CPTs and prove\n",
    "equivalence.\n",
    "\n",
    "**14.11** \\[LG-exercise\\] Consider the family of linear Gaussian networks, as\n",
    "defined on page [LG-network-page](#/).\n",
    "\n",
    "1.  In a two-variable network, let $X_1$ be the parent of $X_2$, let\n",
    "    $X_1$ have a Gaussian prior, and let\n",
    "    ${\\textbf{P}}(X_2{{\\,|\\,}}X_1)$ be a linear\n",
    "    Gaussian distribution. Show that the joint distribution $P(X_1,X_2)$\n",
    "    is a multivariate Gaussian, and calculate its covariance matrix.\n",
    "\n",
    "2.  Prove by induction that the joint distribution for a general linear\n",
    "    Gaussian network on $X_1,\\ldots,X_n$ is also a\n",
    "    multivariate Gaussian.\n",
    "\n",
    "**14.12** \\[multivalued-probit-exercise\\] The probit distribution defined on\n",
    "page [probit-page](#/) describes the probability distribution for a Boolean\n",
    "child, given a single continuous parent.\n",
    "\n",
    "1.  How might the definition be extended to cover multiple continuous\n",
    "    parents?\n",
    "\n",
    "2.  How might it be extended to handle a *multivalued*\n",
    "    child variable? Consider both cases where the child’s values are\n",
    "    ordered (as in selecting a gear while driving, depending on speed,\n",
    "    slope, desired acceleration, etc.) and cases where they are\n",
    "    unordered (as in selecting bus, train, or car to get to work).\n",
    "    (*Hint*: Consider ways to divide the possible values\n",
    "    into two sets, to mimic a Boolean variable.)\n",
    "\n",
    "**14.13** In your local nuclear power station, there is an alarm that senses when\n",
    "a temperature gauge exceeds a given threshold. The gauge measures the\n",
    "temperature of the core. Consider the Boolean variables $A$ (alarm\n",
    "sounds), $F_A$ (alarm is faulty), and $F_G$ (gauge is faulty) and the\n",
    "multivalued nodes $G$ (gauge reading) and $T$ (actual core temperature).\n",
    "\n",
    "1.  Draw a Bayesian network for this domain, given that the gauge is\n",
    "    more likely to fail when the core temperature gets too high.\n",
    "\n",
    "2.  Is your network a polytree? Why or why not?\n",
    "\n",
    "3.  Suppose there are just two possible actual and measured\n",
    "    temperatures, normal and high; the probability that the gauge gives\n",
    "    the correct temperature is $x$ when it is working, but $y$ when it\n",
    "    is faulty. Give the conditional probability table associated with\n",
    "    $G$.\n",
    "\n",
    "4.  Suppose the alarm works correctly unless it is faulty, in which case\n",
    "    it never sounds. Give the conditional probability table associated\n",
    "    with $A$.\n",
    "\n",
    "5.  Suppose the alarm and gauge are working and the alarm sounds.\n",
    "    Calculate an expression for the probability that the temperature of\n",
    "    the core is too high, in terms of the various conditional\n",
    "    probabilities in the network.\n",
    "\n",
    "**14.14** \\[telescope-exercise\\] Two astronomers in different parts of the world\n",
    "make measurements $M_1$ and $M_2$ of the number of stars $N$ in some\n",
    "small region of the sky, using their telescopes. Normally, there is a\n",
    "small possibility $e$ of error by up to one star in each direction. Each\n",
    "telescope can also (with a much smaller probability $f$) be badly out of\n",
    "focus (events $F_1$ and $F_2$), in which case the scientist will\n",
    "undercount by three or more stars (or if $N$ is less than 3, fail to\n",
    "detect any stars at all). Consider the three networks shown in\n",
    "Figure [telescope-nets-figure](#telescope-nets-figure).\n",
    "\n",
    "1.  Which of these Bayesian networks are correct (but not\n",
    "    necessarily efficient) representations of the preceding information?\n",
    "\n",
    "2.  Which is the best network? Explain.\n",
    "\n",
    "3.  Write out a conditional distribution for\n",
    "    ${\\textbf{P}}(M_1{{\\,|\\,}}N)$, for the case where\n",
    "    $N{{\\,{\\in}\\,}}\\{1,2,3\\}$ and $M_1{{\\,{\\in}\\,}}\\{0,1,2,3,4\\}$. Each\n",
    "    entry in the conditional distribution should be expressed as a\n",
    "    function of the parameters $e$ and/or $f$.\n",
    "\n",
    "4.  Suppose $M_1{{\\,{=}\\,}}1$ and $M_2{{\\,{=}\\,}}3$. What are the\n",
    "    *possible* numbers of stars if you assume no prior\n",
    "    constraint on the values of $N$?\n",
    "\n",
    "5.  What is the *most likely* number of stars, given these\n",
    "    observations? Explain how to compute this, or if it is not possible\n",
    "    to compute, explain what additional information is needed and how it\n",
    "    would affect the result.\n",
    "\n",
    "**14.15** Consider the network shown in\n",
    "Figure [telescope-nets-figure](#telescope-nets-figure)(ii), and assume that the\n",
    "two telescopes work identically. $N{{\\,{\\in}\\,}}\\{1,2,3\\}$ and\n",
    "$M_1,M_2{{\\,{\\in}\\,}}\\{0,1,2,3,4\\}$, with the symbolic CPTs as described\n",
    "in Exercise [telescope-exercise](#/). Using the enumeration\n",
    "algorithm (Figure [enumeration-algorithm](#/) on\n",
    "page [enumeration-algorithm](#/)), calculate the probability distribution\n",
    "${\\textbf{P}}(N{{\\,|\\,}}M_1{{\\,{=}\\,}}2,M_2{{\\,{=}\\,}}2)$.\n",
    "\n",
    "<center>\n",
    "<b id=\"telescope-nets-figure\">Figure [telescope-nets-figure]</b> Three possible networks for the telescope problem.\n",
    "</center>\n",
    "\n",
    "![telescope-nets-figure](http://nalinc.github.io/aima-exercises/Jupyter%20notebook/figures/telescope-nets.svg)\n",
    "\n",
    "<center>\n",
    "<b id=\"politics-figure\">Figure [politics-figure]</b> A simple Bayes net with\n",
    "Boolean variables B = {BrokeElectionLaw}, I = {Indicted}, M = {PoliticallyMotivatedProsecutor}, G= {FoundGuilty}, J = {Jailed}.\n",
    "</center>\n",
    "\n",
    "![politics-figure](http://nalinc.github.io/aima-exercises/Jupyter%20notebook/figures/politics.svg)\n",
    "\n",
    "**14.16** Consider the Bayes net shown in Figure [politics-figure](#politics-figure).\n",
    "\n",
    "1.  Which of the following are asserted by the network\n",
    "    *structure*?\n",
    "\n",
    "    1.  ${\\textbf{P}}(B,I,M) = {\\textbf{P}}(B){\\textbf{P}}(I){\\textbf{P}}(M)$.\n",
    "\n",
    "    2.  ${\\textbf{P}}(J{{\\,|\\,}}G) = {\\textbf{P}}(J{{\\,|\\,}}G,I)$.\n",
    "\n",
    "    3.  ${\\textbf{P}}(M{{\\,|\\,}}G,B,I) = {\\textbf{P}}(M{{\\,|\\,}}G,B,I,J)$.\n",
    "\n",
    "2.  Calculate the value of $P(b,i,\\lnot m,g,j)$.\n",
    "\n",
    "3.  Calculate the probability that someone goes to jail given that they\n",
    "    broke the law, have been indicted, and face a politically\n",
    "    motivated prosecutor.\n",
    "\n",
    "4.  A **context-specific independence** (see\n",
    "    page [CSI-page](#/)) allows a variable to be independent of some of\n",
    "    its parents given certain values of others. In addition to the usual\n",
    "    conditional independences given by the graph structure, what\n",
    "    context-specific independences exist in the Bayes net in\n",
    "    Figure [politics-figure](#politics-figure)?\n",
    "\n",
    "5.  Suppose we want to add the variable\n",
    "    $P{{\\,{=}\\,}}{PresidentialPardon}$ to the network; draw the new\n",
    "    network and briefly explain any links you add.\n",
    "\n",
    "**14.17** Consider the Bayes net shown in Figure [politics-figure](#politics-figure).\n",
    "\n",
    "1.  Which, if any, of the following are asserted by the network\n",
    "    *structure* (ignoring the CPTs for now)?\n",
    "\n",
    "    1.  ${\\textbf{P}}(B,I,M) = {\\textbf{P}}(B){\\textbf{P}}(I){\\textbf{P}}(M)$.\n",
    "\n",
    "    2.  ${\\textbf{P}}(J{{\\,|\\,}}G) = {\\textbf{P}}(J{{\\,|\\,}}G,I)$.\n",
    "\n",
    "    3.  ${\\textbf{P}}(M{{\\,|\\,}}G,B,I) = {\\textbf{P}}(M{{\\,|\\,}}G,B,I,J)$.\n",
    "\n",
    "2.  Calculate the value of $P(b,i,m,\\lnot g,j)$.\n",
    "\n",
    "3.  Calculate the probability that someone goes to jail given that they\n",
    "    broke the law, have been indicted, and face a politically\n",
    "    motivated prosecutor.\n",
    "\n",
    "4.  A **context-specific independence** (see\n",
    "    page [CSI-page](#/)) allows a variable to be independent of some of\n",
    "    its parents given certain values of others. In addition to the usual\n",
    "    conditional independences given by the graph structure, what\n",
    "    context-specific independences exist in the Bayes net in\n",
    "    Figure [politics-figure](#politics-figure)?\n",
    "\n",
    "5.  Suppose we want to add the variable\n",
    "    $P{{\\,{=}\\,}}{PresidentialPardon}$ to the network; draw the new\n",
    "    network and briefly explain any links you add.\n",
    "\n",
    "**14.18** \\[VE-exercise\\] Consider the variable elimination algorithm in\n",
    "Figure [elimination-ask-algorithm](#/) (page [elimination-ask-algorithm](#/)).\n",
    "\n",
    "1.  Section [exact-inference-section](#/) applies variable\n",
    "    elimination to the query\n",
    "    $${\\textbf{P}}({Burglary}{{\\,|\\,}}{JohnCalls}{{\\,{=}\\,}}{true},{MaryCalls}{{\\,{=}\\,}}{true})\\ .$$\n",
    "    Perform the calculations indicated and check that the answer\n",
    "    is correct.\n",
    "\n",
    "2.  Count the number of arithmetic operations performed, and compare it\n",
    "    with the number performed by the enumeration algorithm.\n",
    "\n",
    "3.  Suppose a network has the form of a *chain*: a sequence\n",
    "    of Boolean variables $X_1,\\ldots, X_n$ where\n",
    "    ${Parents}(X_i){{\\,{=}\\,}}\\{X_{i-1}\\}$ for $i{{\\,{=}\\,}}2,\\ldots,n$.\n",
    "    What is the complexity of computing\n",
    "    ${\\textbf{P}}(X_1{{\\,|\\,}}X_n{{\\,{=}\\,}}{true})$ using\n",
    "    enumeration? Using variable elimination?\n",
    "\n",
    "4.  Prove that the complexity of running variable elimination on a\n",
    "    polytree network is linear in the size of the tree for any variable\n",
    "    ordering consistent with the network structure.\n",
    "\n",
    "**14.19** \\[bn-complexity-exercise\\] Investigate the complexity of exact inference\n",
    "in general Bayesian networks:\n",
    "\n",
    "1.  Prove that any 3-SAT problem can be reduced to exact inference in a\n",
    "    Bayesian network constructed to represent the particular problem and\n",
    "    hence that exact inference is NP-hard. (*Hint*:\n",
    "    Consider a network with one variable for each proposition symbol,\n",
    "    one for each clause, and one for the conjunction of clauses.)\n",
    "\n",
    "2.  The problem of counting the number of satisfying assignments for a\n",
    "    3-SAT problem is \\#P-complete. Show that exact inference is at least\n",
    "    as hard as this.\n",
    "\n",
    "**14.20** \\[primitive-sampling-exercise\\] Consider the problem of generating a\n",
    "random sample from a specified distribution on a single variable. Assume\n",
    "you have a random number generator that returns a random number\n",
    "uniformly distributed between 0 and 1.\n",
    "\n",
    "1.  Let $X$ be a discrete variable with\n",
    "    $P(X{{\\,{=}\\,}}x_i){{\\,{=}\\,}}p_i$ for\n",
    "    $i{{\\,{\\in}\\,}}\\{1,\\ldots,k\\}$. The **cumulative distribution** of $X$ gives the probability\n",
    "    that $X{{\\,{\\in}\\,}}\\{x_1,\\ldots,x_j\\}$ for each possible $j$. (See\n",
    "    also Appendix [math-appendix].) Explain how to\n",
    "    calculate the cumulative distribution in $O(k)$ time and how to\n",
    "    generate a single sample of $X$ from it. Can the latter be done in\n",
    "    less than $O(k)$ time?\n",
    "\n",
    "2.  Now suppose we want to generate $N$ samples of $X$, where $N\\gg k$.\n",
    "    Explain how to do this with an expected run time per sample that is\n",
    "    *constant* (i.e., independent of $k$).\n",
    "\n",
    "3.  Now consider a continuous-valued variable with a parameterized\n",
    "    distribution (e.g., Gaussian). How can samples be generated from\n",
    "    such a distribution?\n",
    "\n",
    "4.  Suppose you want to query a continuous-valued variable and you are\n",
    "    using a sampling algorithm such as LIKELIHOODWEIGHTING to do the inference. How would\n",
    "    you have to modify the query-answering process?\n",
    "\n",
    "**14.21** Consider the query\n",
    "${\\textbf{P}}({Rain}{{\\,|\\,}}{Sprinkler}{{\\,{=}\\,}}{true},{WetGrass}{{\\,{=}\\,}}{true})$\n",
    "in Figure [rain-clustering-figure](#/)(a)\n",
    "(page [rain-clustering-figure](#/)) and how Gibbs sampling can answer it.\n",
    "\n",
    "1.  How many states does the Markov chain have?\n",
    "\n",
    "2.  Calculate the **transition matrix**\n",
    "    ${\\textbf{Q}}$ containing\n",
    "    $q({\\textbf{y}} \\rightarrow {{\\textbf{y}}'})$\n",
    "    for all ${\\textbf{y}}$, ${\\textbf{y}}'$.\n",
    "\n",
    "3.  What does ${\\textbf{ Q}}^2$, the square of the\n",
    "    transition matrix, represent?\n",
    "\n",
    "4.  What about ${\\textbf{Q}}^n$ as $n\\to \\infty$?\n",
    "\n",
    "5.  Explain how to do probabilistic inference in Bayesian networks,\n",
    "    assuming that ${\\textbf{Q}}^n$ is available. Is this a\n",
    "    practical way to do inference?\n",
    "\n",
    "**14.22** \\[gibbs-proof-exercise\\] This exercise explores the stationary\n",
    "distribution for Gibbs sampling methods.\n",
    "\n",
    "1.  The convex composition $[\\alpha, q_1; 1-\\alpha, q_2]$ of $q_1$ and\n",
    "    $q_2$ is a transition probability distribution that first chooses\n",
    "    one of $q_1$ and $q_2$ with probabilities $\\alpha$ and $1-\\alpha$,\n",
    "    respectively, and then applies whichever is chosen. Prove that if\n",
    "    $q_1$ and $q_2$ are in detailed balance with $\\pi$, then their\n",
    "    convex composition is also in detailed balance with $\\pi$.\n",
    "    (*Note*: this result justifies a variant of GIBBS-ASK in which\n",
    "    variables are chosen at random rather than sampled in a\n",
    "    fixed sequence.)\n",
    "\n",
    "2.  Prove that if each of $q_1$ and $q_2$ has $\\pi$ as its stationary\n",
    "    distribution, then the sequential composition\n",
    "    $q {{\\,{=}\\,}}q_1 \\circ q_2$ also has $\\pi$ as its\n",
    "    stationary distribution.\n",
    "\n",
    "**14.23** \\[MH-exercise\\] The **Metropolis--Hastings** algorithm is a member of the MCMC family; as such,\n",
    "it is designed to generate samples $\\textbf{x}$ (eventually) according to target\n",
    "probabilities $\\pi(\\textbf{x})$. (Typically we are interested in sampling from\n",
    "$\\pi(\\textbf{x}){{\\,{=}\\,}}P(\\textbf{x}{{\\,|\\,}}\\textbf{e})$.) Like simulated annealing,\n",
    "Metropolis–Hastings operates in two stages. First, it samples a new\n",
    "state $\\textbf{x'}$ from a **proposal distribution** $q(\\textbf{x'}{{\\,|\\,}}\\textbf{x})$, given the current state $\\textbf{x}$.\n",
    "Then, it probabilistically accepts or rejects $\\textbf{x'}$ according to the **acceptance probability**\n",
    "$$\\alpha(\\textbf{x'}{{\\,|\\,}}\\textbf{x}) = \\min\\ \\left(1,\\frac{\\pi(\\textbf{x'})q(\\textbf{x}{{\\,|\\,}}\\textbf{x'})}{\\pi(\\textbf{x})q(\\textbf{x'}{{\\,|\\,}}\\textbf{x})}  \\right)\\ .$$\n",
    "If the proposal is rejected, the state remains at $\\textbf{x}$.\n",
    "\n",
    "1.  Consider an ordinary Gibbs sampling step for a specific variable\n",
    "    $X_i$. Show that this step, considered as a proposal, is guaranteed\n",
    "    to be accepted by Metropolis–Hastings. (Hence, Gibbs sampling is a\n",
    "    special case of Metropolis–Hastings.)\n",
    "\n",
    "2.  Show that the two-step process above, viewed as a transition\n",
    "    probability distribution, is in detailed balance with $\\pi$.\n",
    "\n",
    "**14.24** \\[soccer-rpm-exercise\\]Three soccer teams $A$, $B$, and $C$, play each\n",
    "other once. Each match is between two teams, and can be won, drawn, or\n",
    "lost. Each team has a fixed, unknown degree of quality—an integer\n",
    "ranging from 0 to 3—and the outcome of a match depends probabilistically\n",
    "on the difference in quality between the two teams.\n",
    "\n",
    "1.  Construct a relational probability model to describe this domain,\n",
    "    and suggest numerical values for all the necessary\n",
    "    probability distributions.\n",
    "\n",
    "2.  Construct the equivalent Bayesian network for the three matches.\n",
    "\n",
    "3.  Suppose that in the first two matches $A$ beats $B$ and draws with\n",
    "    $C$. Using an exact inference algorithm of your choice, compute the\n",
    "    posterior distribution for the outcome of the third match.\n",
    "\n",
    "4.  Suppose there are $n$ teams in the league and we have the results\n",
    "    for all but the last match. How does the complexity of predicting\n",
    "    the last game vary with $n$?\n",
    "\n",
    "5.  Investigate the application of MCMC to this problem. How quickly\n",
    "    does it converge in practice and how well does it scale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
