{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Making Complex Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**17.1** \\[mdp-model-exercise\\]For the $4\\times 3$ world shown in\n",
    "Figure [sequential-decision-world-figure](#/), calculate\n",
    "which squares can be reached from (1,1) by the action sequence\n",
    "$[{Up},{Up},{Right},{Right},{Right}]$ and with what\n",
    "probabilities. Explain how this computation is related to the prediction\n",
    "task (see Section [general-filtering-section](#/)) for a\n",
    "hidden Markov model.\n",
    "\n",
    "**17.2** \\[mdp-model-exercise\\]For the $4\\times 3$ world shown in\n",
    "Figure [sequential-decision-world-figure](#/), calculate\n",
    "which squares can be reached from (1,1) by the action sequence\n",
    "$[{Right},{Right},{Right},{Up},{Up}]$ and with what\n",
    "probabilities. Explain how this computation is related to the prediction\n",
    "task (see Section [general-filtering-section](#/)) for a\n",
    "hidden Markov model.\n",
    "\n",
    "**17.3** Select a specific member of the set of policies that are optimal for\n",
    "$R(s)>0$ as shown in\n",
    "Figure [sequential-decision-policies-figure](#/)(b), and\n",
    "calculate the fraction of time the agent spends in each state, in the\n",
    "limit, if the policy is executed forever. (*Hint*:\n",
    "Construct the state-to-state transition probability matrix corresponding\n",
    "to the policy and see\n",
    "Exercise [markov-convergence-exercise](#/).)\n",
    "\n",
    "**17.4** \\[nonseparable-exercise\\]Suppose that we define the utility of a state\n",
    "sequence to be the *maximum* reward obtained in any state\n",
    "in the sequence. Show that this utility function does not result in\n",
    "stationary preferences between state sequences. Is it still possible to\n",
    "define a utility function on states such that MEU decision making gives\n",
    "optimal behavior?\n",
    "\n",
    "**17.5** Can any finite search problem be translated exactly into a Markov\n",
    "decision problem such that an optimal solution of the latter is also an\n",
    "optimal solution of the former? If so, explain *precisely*\n",
    "how to translate the problem and how to translate the solution back; if\n",
    "not, explain *precisely* why not (i.e., give a\n",
    "counterexample).\n",
    "\n",
    "**17.6** \\[reward-equivalence-exercise\\] Sometimes MDPs are formulated with a\n",
    "reward function $R(s,a)$ that depends on the action taken or with a\n",
    "reward function $R(s,a,s')$ that also depends on the outcome state.\n",
    "\n",
    "1.  Write the Bellman equations for these formulations.\n",
    "\n",
    "2.  Show how an MDP with reward function $R(s,a,s')$ can be transformed\n",
    "    into a different MDP with reward function $R(s,a)$, such that\n",
    "    optimal policies in the new MDP correspond exactly to optimal\n",
    "    policies in the original MDP.\n",
    "\n",
    "3.  Now do the same to convert MDPs with $R(s,a)$ into MDPs with $R(s)$.\n",
    "\n",
    "**17.7** \\[threshold-cost-exercise\\]For the environment shown in\n",
    "Figure [sequential-decision-world-figure](#/), find all the\n",
    "threshold values for $R(s)$ such that the optimal policy changes when\n",
    "the threshold is crossed. You will need a way to calculate the optimal\n",
    "policy and its value for fixed $R(s)$. (*Hint*: Prove that\n",
    "the value of any fixed policy varies linearly with $R(s)$.)\n",
    "\n",
    "**17.8** \\[vi-contraction-exercise\\]\n",
    "Equation ([vi-contraction-equation](#/)) on\n",
    "page [vi-contraction-equation](#/) states that the Bellman operator is a contraction.\n",
    "\n",
    "1.  Show that, for any functions $f$ and $g$,\n",
    "    $$|\\max_a f(a) - \\max_a g(a)| \\leq \\max_a |f(a) - g(a)|\\ .$$\n",
    "\n",
    "2.  Write out an expression for $|(B\\,U_i - B\\,U'_i)(s)|$ and then apply\n",
    "    the result from (a) to complete the proof that the Bellman operator\n",
    "    is a contraction.\n",
    "\n",
    "**17.9** This exercise considers two-player MDPs that correspond to zero-sum,\n",
    "turn-taking games like those in\n",
    "Chapter [game-playing-chapter](#/). Let the players be $A$\n",
    "and $B$, and let $R(s)$ be the reward for player $A$ in state $s$. (The\n",
    "reward for $B$ is always equal and opposite.)\n",
    "\n",
    "1.  Let $U_A(s)$ be the utility of state $s$ when it is $A$’s turn to\n",
    "    move in $s$, and let $U_B(s)$ be the utility of state $s$ when it is\n",
    "    $B$’s turn to move in $s$. All rewards and utilities are calculated\n",
    "    from $A$’s point of view (just as in a minimax game tree). Write\n",
    "    down Bellman equations defining $U_A(s)$ and $U_B(s)$.\n",
    "\n",
    "2.  Explain how to do two-player value iteration with these equations,\n",
    "    and define a suitable termination criterion.\n",
    "\n",
    "3.  Consider the game described in\n",
    "    Figure [line-game4-figure](#/) on page [line-game4-figure](#/).\n",
    "    Draw the state space (rather than the game tree), showing the moves\n",
    "    by $A$ as solid lines and moves by $B$ as dashed lines. Mark each\n",
    "    state with $R(s)$. You will find it helpful to arrange the states\n",
    "    $(s_A,s_B)$ on a two-dimensional grid, using $s_A$ and $s_B$ as\n",
    "    “coordinates.”\n",
    "\n",
    "4.  Now apply two-player value iteration to solve this game, and derive\n",
    "    the optimal policy.\n",
    "\n",
    "<center>\n",
    "<b id=\"grid-mdp-figure\">Figure [grid-mdp-figure]</b> (a) $3 \\times 3$ world for Exercise [3x3-mdp-exercise](#/). The reward for each state is indicated. The upper right square is a terminal state. (b) $101 \\times 3$ world for Exercise [101x3-mdp-exercise](#/) (omitting 93 identical columns in the middle). \n",
    "The start state has reward 0.\n",
    "</center>\n",
    "\n",
    "![grid-mdp-figure](https://cdn.rawgit.com/Nalinc/aima-exercises/notebooks/Jupyter%20notebook/figures/grid-mdp-figure.svg)\n",
    "\n",
    "**17.10** \\[3x3-mdp-exercise\\] Consider the $3 \\times 3$ world shown in\n",
    "Figure [grid-mdp-figure](#grid-mdp-figure)(a). The transition model is the\n",
    "same as in the $4\\times 3$\n",
    "Figure [sequential-decision-world-figure](#/): 80% of the\n",
    "time the agent goes in the direction it selects; the rest of the time it\n",
    "moves at right angles to the intended direction.\n",
    "\n",
    "Implement value iteration for this world for each value of $r$ below.\n",
    "Use discounted rewards with a discount factor of 0.99. Show the policy\n",
    "obtained in each case. Explain intuitively why the value of $r$ leads to\n",
    "each policy.\n",
    "\n",
    "1.  $r = -100$\n",
    "\n",
    "2.  $r = -3$\n",
    "\n",
    "3.  $r = 0$\n",
    "\n",
    "4.  $r = +3$\n",
    "\n",
    "**17.11** \\[101x3-mdp-exercise\\] Consider the $101 \\times 3$ world shown in\n",
    "Figure [grid-mdp-figure](#grid-mdp-figure)(b). In the start state the agent\n",
    "has a choice of two deterministic actions, *Up* or\n",
    "*Down*, but in the other states the agent has one\n",
    "deterministic action, *Right*. Assuming a discounted reward\n",
    "function, for what values of the discount $\\gamma$ should the agent\n",
    "choose *Up* and for which *Down*? Compute the\n",
    "utility of each action as a function of $\\gamma$. (Note that this simple\n",
    "example actually reflects many real-world situations in which one must\n",
    "weigh the value of an immediate action versus the potential continual\n",
    "long-term consequences, such as choosing to dump pollutants into a\n",
    "lake.)\n",
    "\n",
    "**17.12** Consider an undiscounted MDP having three states, (1, 2, 3), with\n",
    "rewards $-1$, $-2$, $0$, respectively. State 3 is a terminal state. In\n",
    "states 1 and 2 there are two possible actions: $a$ and $b$. The\n",
    "transition model is as follows:\n",
    "\n",
    "-   In state 1, action $a$ moves the agent to state 2 with probability\n",
    "    0.8 and makes the agent stay put with probability 0.2.\n",
    "\n",
    "-   In state 2, action $a$ moves the agent to state 1 with probability\n",
    "    0.8 and makes the agent stay put with probability 0.2.\n",
    "\n",
    "-   In either state 1 or state 2, action $b$ moves the agent to state 3\n",
    "    with probability 0.1 and makes the agent stay put with\n",
    "    probability 0.9.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1.  What can be determined *qualitatively* about the\n",
    "    optimal policy in states 1 and 2?\n",
    "\n",
    "2.  Apply policy iteration, showing each step in full, to determine the\n",
    "    optimal policy and the values of states 1 and 2. Assume that the\n",
    "    initial policy has action $b$ in both states.\n",
    "\n",
    "3.  What happens to policy iteration if the initial policy has action\n",
    "    $a$ in both states? Does discounting help? Does the optimal policy\n",
    "    depend on the discount factor?\n",
    "\n",
    "**17.13** Consider the $4\\times 3$ world shown in\n",
    "Figure [sequential-decision-world-figure](#/).\n",
    "\n",
    "1.  Implement an environment simulator for this environment, such that\n",
    "    the specific geography of the environment is easily altered. Some\n",
    "    code for doing this is already in the online code repository.\n",
    "\n",
    "2.  Create an agent that uses policy iteration, and measure its\n",
    "    performance in the environment simulator from various\n",
    "    starting states. Perform several experiments from each starting\n",
    "    state, and compare the average total reward received per run with\n",
    "    the utility of the state, as determined by your algorithm.\n",
    "\n",
    "3.  Experiment with increasing the size of the environment. How does the\n",
    "    run time for policy iteration vary with the size of the environment?\n",
    "\n",
    "**17.14** \\[policy-loss-exercise\\]How can the value determination algorithm be\n",
    "used to calculate the expected loss experienced by an agent using a\n",
    "given set of utility estimates ${U}$ and an estimated\n",
    "model ${P}$, compared with an agent using correct values?\n",
    "\n",
    "**17.15** \\[4x3-pomdp-exercise\\] Let the initial belief state $b_0$ for the\n",
    "$4\\times 3$ POMDP on page [4x3-pomdp-page](#/) be the uniform distribution\n",
    "over the nonterminal states, i.e.,\n",
    "$< \\frac{1}{9},\\frac{1}{9},\\frac{1}{9},\\frac{1}{9},\\frac{1}{9},\\frac{1}{9},\\frac{1}{9},\\frac{1}{9},\\frac{1}{9},0,0 >$.\n",
    "Calculate the exact belief state $b_1$ after the agent moves and its\n",
    "sensor reports 1 adjacent wall. Also calculate $b_2$ assuming that the\n",
    "same thing happens again.\n",
    "\n",
    "**17.16** What is the time complexity of $d$ steps of POMDP value iteration for a\n",
    "sensorless environment?\n",
    "\n",
    "**17.17** \\[2state-pomdp-exercise\\] Consider a version of the two-state POMDP on\n",
    "page [2state-pomdp-page](#/) in which the sensor is 90% reliable in state 0 but\n",
    "provides no information in state 1 (that is, it reports 0 or 1 with\n",
    "equal probability). Analyze, either qualitatively or quantitatively, the\n",
    "utility function and the optimal policy for this problem.\n",
    "\n",
    "**17.18** \\[dominant-equilibrium-exercise\\]Show that a dominant strategy\n",
    "equilibrium is a Nash equilibrium, but not vice versa.\n",
    "\n",
    "**17.19** In the children’s game of rock–paper–scissors each player reveals at the\n",
    "same time a choice of rock, paper, or scissors. Paper wraps rock, rock\n",
    "blunts scissors, and scissors cut paper. In the extended version\n",
    "rock–paper–scissors–fire–water, fire beats rock, paper, and scissors;\n",
    "rock, paper, and scissors beat water; and water beats fire. Write out\n",
    "the payoff matrix and find a mixed-strategy solution to this game.\n",
    "\n",
    "**17.20** Solve the game of *three*-finger Morra.\n",
    "\n",
    "**17.21** In the *Prisoner’s Dilemma*, consider the case where after\n",
    "each round, Alice and Bob have probability $X$ meeting again. Suppose\n",
    "both players choose the perpetual punishment strategy (where each will\n",
    "choose ${refuse}$ unless the other player has ever played\n",
    "${testify}$). Assume neither player has played ${testify}$ thus far.\n",
    "What is the expected future total payoff for choosing to ${testify}$\n",
    "versus ${refuse}$ when $X = .2$? How about when $X = .05$? For what\n",
    "value of $X$ is the expected future total payoff the same whether one\n",
    "chooses to ${testify}$ or ${refuse}$ in the current round?\n",
    "\n",
    "**17.22** The following payoff matrix, from @Blinder:1983 by way of @Bernstein:1996, shows a game between\n",
    "politicians and the Federal Reserve.\n",
    "\n",
    "|     | Fed: contract | Fed: do nothing  | Fed: expand     |\n",
    "| --- | --- | --- | --- |\n",
    "| **Pol: contract**    | $F=7, P=1$ | $F=9,P=4$  | $F=6,P=6$   |\n",
    "| **Pol: do nothing**  | $F=8, P=2$ | $F=5,P=5$  | $F=4,P=9$   |\n",
    "| **Pol: expand**      | $F=3, P=3$ | $F=2,P=7$  | $F=1,P=8$   |\n",
    "\n",
    "Politicians can expand or contract fiscal policy, while the Fed can\n",
    "expand or contract monetary policy. (And of course either side can\n",
    "choose to do nothing.) Each side also has preferences for who should do\n",
    "what—neither side wants to look like the bad guys. The payoffs shown are\n",
    "simply the rank orderings: 9 for first choice through 1 for last choice.\n",
    "Find the Nash equilibrium of the game in pure strategies. Is this a\n",
    "Pareto-optimal solution? You might wish to analyze the policies of\n",
    "recent administrations in this light.\n",
    "\n",
    "**17.23** A Dutch auction is similar in an English auction, but rather than\n",
    "starting the bidding at a low price and increasing, in a Dutch auction\n",
    "the seller starts at a high price and gradually lowers the price until\n",
    "some buyer is willing to accept that price. (If multiple bidders accept\n",
    "the price, one is arbitrarily chosen as the winner.) More formally, the\n",
    "seller begins with a price $p$ and gradually lowers $p$ by increments of\n",
    "$d$ until at least one buyer accepts the price. Assuming all bidders act\n",
    "rationally, is it true that for arbitrarily small $d$, a Dutch auction\n",
    "will always result in the bidder with the highest value for the item\n",
    "obtaining the item? If so, show mathematically why. If not, explain how\n",
    "it may be possible for the bidder with highest value for the item not to\n",
    "obtain it.\n",
    "\n",
    "**17.24** Imagine an auction mechanism that is just like an ascending-bid auction,\n",
    "except that at the end, the winning bidder, the one who bid $b_{max}$,\n",
    "pays only $b_{max}/2$ rather than $b_{max}$. Assuming all agents are\n",
    "rational, what is the expected revenue to the auctioneer for this\n",
    "mechanism, compared with a standard ascending-bid auction?\n",
    "\n",
    "**17.25** Teams in the National Hockey League historically received 2 points for\n",
    "winning a game and 0 for losing. If the game is tied, an overtime period\n",
    "is played; if nobody wins in overtime, the game is a tie and each team\n",
    "gets 1 point. But league officials felt that teams were playing too\n",
    "conservatively in overtime (to avoid a loss), and it would be more\n",
    "exciting if overtime produced a winner. So in 1999 the officials\n",
    "experimented in mechanism design: the rules were changed, giving a team\n",
    "that loses in overtime 1 point, not 0. It is still 2 points for a win\n",
    "and 1 for a tie.\n",
    "\n",
    "1.  Was hockey a zero-sum game before the rule change? After?\n",
    "\n",
    "2.  Suppose that at a certain time $t$ in a game, the home team has\n",
    "    probability $p$ of winning in regulation time, probability $0.78-p$\n",
    "    of losing, and probability 0.22 of going into overtime, where they\n",
    "    have probability $q$ of winning, $.9-q$ of losing, and .1 of tying.\n",
    "    Give equations for the expected value for the home and\n",
    "    visiting teams.\n",
    "\n",
    "3.  Imagine that it were legal and ethical for the two teams to enter\n",
    "    into a pact where they agree that they will skate to a tie in\n",
    "    regulation time, and then both try in earnest to win in overtime.\n",
    "    Under what conditions, in terms of $p$ and $q$, would it be rational\n",
    "    for both teams to agree to this pact?\n",
    "\n",
    "4.  @Longley+Sankaran:2005 report that since the rule change, the percentage of games with a\n",
    "    winner in overtime went up 18.2%, as desired, but the percentage of\n",
    "    overtime games also went up 3.6%. What does that suggest about\n",
    "    possible collusion or conservative play after the rule change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
