{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Probabilistic Reasoning over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15.1** \\[state-augmentation-exercise\\] Show that any second-order Markov\n",
    "process can be rewritten as a first-order Markov process with an\n",
    "augmented set of state variables. Can this always be done\n",
    "*parsimoniously*, i.e., without increasing the number of\n",
    "parameters needed to specify the transition model?\n",
    "\n",
    "**15.2** \\[markov-convergence-exercise\\] In this exercise, we examine what\n",
    "happens to the probabilities in the umbrella world in the limit of long\n",
    "time sequences.\n",
    "\n",
    "1.  Suppose we observe an unending sequence of days on which the\n",
    "    umbrella appears. Show that, as the days go by, the probability of\n",
    "    rain on the current day increases monotonically toward a\n",
    "    fixed point. Calculate this fixed point.\n",
    "\n",
    "2.  Now consider *forecasting* further and further into the\n",
    "    future, given just the first two umbrella observations. First,\n",
    "    compute the probability $P(r_{2+k}|u_1,u_2)$ for\n",
    "    $k{{\\,{=}\\,}}1\\ldots {20}$ and plot the results. You should see that\n",
    "    the probability converges towards a fixed point. Prove that the\n",
    "    exact value of this fixed point is 0.5.\n",
    "\n",
    "**15.3** \\[island-exercise\\] This exercise develops a space-efficient variant of\n",
    "the forward–backward algorithm described in\n",
    "Figure [forward-backward-algorithm](#/) (page [forward-backward-algorithm](#/)).\n",
    "We wish to compute ${\\textbf{P}}(\\textbf{X}_k|\\textbf{e}_{1:t})$ for\n",
    "$k{{\\,{=}\\,}}1,\\ldots ,t$. This will be done with a divide-and-conquer\n",
    "approach.\n",
    "\n",
    "1.  Suppose, for simplicity, that $t$ is odd, and let the halfway point\n",
    "    be $h{{\\,{=}\\,}}(t+1)/2$. Show that\n",
    "    ${\\textbf{P}}(\\textbf{X}_k|\\textbf{e}_{1:t})$ can be computed for\n",
    "    $k{{\\,{=}\\,}}1,\\ldots ,h$ given just the initial forward message\n",
    "    $\\textbf{f}_{1:0}$, the backward message $\\textbf{b}_{h+1:t}$, and the evidence\n",
    "    $\\textbf{e}_{1:h}$.\n",
    "\n",
    "2.  Show a similar result for the second half of the sequence.\n",
    "\n",
    "3.  Given the results of (a) and (b), a recursive divide-and-conquer\n",
    "    algorithm can be constructed by first running forward along the\n",
    "    sequence and then backward from the end, storing just the required\n",
    "    messages at the middle and the ends. Then the algorithm is called on\n",
    "    each half. Write out the algorithm in detail.\n",
    "\n",
    "4.  Compute the time and space complexity of the algorithm as a function\n",
    "    of $t$, the length of the sequence. How does this change if we\n",
    "    divide the input into more than two pieces?\n",
    "\n",
    "**15.4** \\[flawed-viterbi-exercise\\] On page [flawed-viterbi-page](#/), we outlined a flawed\n",
    "procedure for finding the most likely state sequence, given an\n",
    "observation sequence. The procedure involves finding the most likely\n",
    "state at each time step, using smoothing, and returning the sequence\n",
    "composed of these states. Show that, for some temporal probability\n",
    "models and observation sequences, this procedure returns an impossible\n",
    "state sequence (i.e., the posterior probability of the sequence is\n",
    "zero).\n",
    "\n",
    "**15.5** \\[hmm-likelihood-exercise\\]\n",
    "Equation ([matrix-filtering-equation](#/)) describes the\n",
    "filtering process for the matrix formulation of HMMs. Give a similar\n",
    "equation for the calculation of likelihoods, which was described\n",
    "generically in Equation ([forward-likelihood-equation](#/)).\n",
    "\n",
    "**15.6** Consider the vacuum worlds of\n",
    "Figure [vacuum-maze-ch4-figure](#/) (perfect sensing) and\n",
    "Figure [vacuum-maze-hmm2-figure](#/) (noisy sensing). Suppose\n",
    "that the robot receives an observation sequence such that, with perfect\n",
    "sensing, there is exactly one possible location it could be in. Is this\n",
    "location necessarily the most probable location under noisy sensing for\n",
    "sufficiently small noise probability $\\epsilon$? Prove your claim or\n",
    "find a counterexample.\n",
    "\n",
    "**15.7** \\[hmm-robust-exercise\\] In\n",
    "Section [hmm-localization-section](#/), the prior\n",
    "distribution over locations is uniform and the transition model assumes\n",
    "an equal probability of moving to any neighboring square. What if those\n",
    "assumptions are wrong? Suppose that the initial location is actually\n",
    "chosen uniformly from the northwest quadrant of the room and the action\n",
    "actually tends to move southeast\\[hmm-robot-southeast-page\\]. Keeping\n",
    "the HMM model fixed, explore the effect on localization and path\n",
    "accuracy as the southeasterly tendency increases, for different values\n",
    "of $\\epsilon$.\n",
    "\n",
    "**15.8** \\[roomba-viterbi-exercise\\] Consider a version of the vacuum robot\n",
    "(page [vacuum-maze-hmm2-figure](#/)) that has the policy of going straight for as long\n",
    "as it can; only when it encounters an obstacle does it change to a new\n",
    "(randomly selected) heading. To model this robot, each state in the\n",
    "model consists of a *(location, heading)* pair. Implement\n",
    "this model and see how well the Viterbi algorithm can track a robot with\n",
    "this model. The robot’s policy is more constrained than the random-walk\n",
    "robot; does that mean that predictions of the most likely path are more\n",
    "accurate?\n",
    "\n",
    "**15.9** We have described three policies for the vacuum robot: (1) a uniform\n",
    "random walk, (2) a bias for wandering southeast, as described in\n",
    "Exercise [hmm-robust-exercise](#/), and (3) the policy\n",
    "described in Exercise [roomba-viterbi-exercise](#/). Suppose\n",
    "an observer is given the observation sequence from a vacuum robot, but\n",
    "is not sure which of the three policies the robot is following. What\n",
    "approach should the observer use to find the most likely path, given the\n",
    "observations? Implement the approach and test it. How much does the\n",
    "localization accuracy suffer, compared to the case in which the observer\n",
    "knows which policy the robot is following?\n",
    "\n",
    "**15.10** This exercise is concerned with filtering in an environment with no\n",
    "landmarks. Consider a vacuum robot in an empty room, represented by an\n",
    "$n \\times m$ rectangular grid. The robot’s location is hidden; the only\n",
    "evidence available to the observer is a noisy location sensor that gives\n",
    "an approximation to the robot’s location. If the robot is at location\n",
    "$(x, y)$ then with probability .1 the sensor gives the correct location,\n",
    "with probability .05 each it reports one of the 8 locations immediately\n",
    "surrounding $(x, y)$, with probability .025 each it reports one of the\n",
    "16 locations that surround those 8, and with the remaining probability\n",
    "of .1 it reports “no reading.” The robot’s policy is to pick a direction\n",
    "and follow it with probability .8 on each step; the robot switches to a\n",
    "randomly selected new heading with probability .2 (or with probability 1\n",
    "if it encounters a wall). Implement this as an HMM and do filtering to\n",
    "track the robot. How accurately can we track the robot’s path?\n",
    "\n",
    "**15.11** This exercise is concerned with filtering in an environment with no\n",
    "landmarks. Consider a vacuum robot in an empty room, represented by an\n",
    "$n \\times m$ rectangular grid. The robot’s location is hidden; the only\n",
    "evidence available to the observer is a noisy location sensor that gives\n",
    "an approximation to the robot’s location. If the robot is at location\n",
    "$(x, y)$ then with probability .1 the sensor gives the correct location,\n",
    "with probability .05 each it reports one of the 8 locations immediately\n",
    "surrounding $(x, y)$, with probability .025 each it reports one of the\n",
    "16 locations that surround those 8, and with the remaining probability\n",
    "of .1 it reports “no reading.” The robot’s policy is to pick a direction\n",
    "and follow it with probability .7 on each step; the robot switches to a\n",
    "randomly selected new heading with probability .3 (or with probability 1\n",
    "if it encounters a wall). Implement this as an HMM and do filtering to\n",
    "track the robot. How accurately can we track the robot’s path?\n",
    "\n",
    "<center>\n",
    "<b id=\"switching-kf-figure\">Figure [switching-kf-figure]</b> A Bayesian network representation of a switching Kalman filter. The switching variable \\(S_t\\) is a discrete state variable whose value determines\n",
    "the transition model for the continuous state variables $\\textbf{X}_t$.\n",
    "For any discrete state *i*, the transition model\n",
    "$\\textbf{P}(\\textbf{X}_{t+1}|\\textbf{X}_t,S_t= i)$ is a linear Gaussian model, just as in a\n",
    "regular Kalman filter. The transition model for the discrete state,\n",
    "$\\textbf{P}(S_{t+1}|S_t)$, can be thought of as a matrix, as in a hidden\n",
    "Markov model.\n",
    "</center>\n",
    "\n",
    "![switching-kf-figure](http://nalinc.github.io/aima-exercises/Jupyter%20notebook/figures/switching-kf.svg)\n",
    "\n",
    "**15.12** \\[switching-kf-exercise\\] Often, we wish to monitor a continuous-state\n",
    "system whose behavior switches unpredictably among a set of $k$ distinct\n",
    "“modes.” For example, an aircraft trying to evade a missile can execute\n",
    "a series of distinct maneuvers that the missile may attempt to track. A\n",
    "Bayesian network representation of such a **switching Kalman\n",
    "filter** model is shown in\n",
    "Figure [switching-kf-figure](#switching-kf-figure).\n",
    "\n",
    "1.  Suppose that the discrete state $S_t$ has $k$ possible values and\n",
    "    that the prior continuous state estimate\n",
    "    ${\\textbf{P}}(\\textbf{X}_0)$ is a multivariate\n",
    "    Gaussian distribution. Show that the prediction\n",
    "    ${\\textbf{P}}(\\textbf{X}_1)$ is a **mixture of\n",
    "    Gaussians**—that is, a weighted sum of Gaussians such\n",
    "    that the weights sum to 1.\n",
    "\n",
    "2.  Show that if the current continuous state estimate\n",
    "    ${\\textbf{P}}(\\textbf{X}_t|\\textbf{e}_{1:t})$ is a mixture of $m$ Gaussians,\n",
    "    then in the general case the updated state estimate\n",
    "    ${\\textbf{P}}(\\textbf{X}_{t+1}|\\textbf{e}_{1:t+1})$ will be a mixture of\n",
    "    $km$ Gaussians.\n",
    "\n",
    "3.  What aspect of the temporal process do the weights in the Gaussian\n",
    "    mixture represent?\n",
    "\n",
    "The results in (a) and (b) show that the representation of the posterior\n",
    "grows without limit even for switching Kalman filters, which are among\n",
    "the simplest hybrid dynamic models.\n",
    "\n",
    "**15.13** \\[kalman-update-exercise\\] Complete the missing step in the derivation\n",
    "of Equation ([kalman-one-step-equation](#/)) on\n",
    "page [kalman-one-step-equation](#/), the first update step for the one-dimensional Kalman\n",
    "filter.\n",
    "\n",
    "**15.14** \\[kalman-variance-exercise\\] Let us examine the behavior of the variance\n",
    "update in Equation ([kalman-univariate-equation](#/))\n",
    "(page [kalman-univariate-equation](#/)).\n",
    "\n",
    "1.  Plot the value of $\\sigma_t^2$ as a function of $t$, given various\n",
    "    values for $\\sigma_x^2$ and $\\sigma_z^2$.\n",
    "\n",
    "2.  Show that the update has a fixed point $\\sigma^2$ such that\n",
    "    $\\sigma_t^2 \\rightarrow \\sigma^2$ as $t \\rightarrow \\infty$, and\n",
    "    calculate the value of $\\sigma^2$.\n",
    "\n",
    "3.  Give a qualitative explanation for what happens as\n",
    "    $\\sigma_x^2\\rightarrow 0$ and as $\\sigma_z^2\\rightarrow 0$.\n",
    "\n",
    "**15.15** \\[sleep1-exercise\\] A professor wants to know if students are getting\n",
    "enough sleep. Each day, the professor observes whether the students\n",
    "sleep in class, and whether they have red eyes. The professor has the\n",
    "following domain theory:\n",
    "\n",
    "-   The prior probability of getting enough sleep, with no observations,\n",
    "    is 0.7.\n",
    "\n",
    "-   The probability of getting enough sleep on night $t$ is 0.8 given\n",
    "    that the student got enough sleep the previous night, and 0.3\n",
    "    if not.\n",
    "\n",
    "-   The probability of having red eyes is 0.2 if the student got enough\n",
    "    sleep, and 0.7 if not.\n",
    "\n",
    "-   The probability of sleeping in class is 0.1 if the student got\n",
    "    enough sleep, and 0.3 if not.\n",
    "\n",
    "Formulate this information as a dynamic Bayesian network that the\n",
    "professor could use to filter or predict from a sequence of\n",
    "observations. Then reformulate it as a hidden Markov model that has only\n",
    "a single observation variable. Give the complete probability tables for\n",
    "the model.\n",
    "\n",
    "**15.16** \\[sleep1-exercise\\] A professor wants to know if students are getting\n",
    "enough sleep. Each day, the professor observes whether the students\n",
    "sleep in class, and whether they have red eyes. The professor has the\n",
    "following domain theory:\n",
    "\n",
    "-   The prior probability of getting enough sleep, with no observations,\n",
    "    is 0.6.\n",
    "\n",
    "-   The probability of getting enough sleep on night $t$ is 0.8 given\n",
    "    that the student got enough sleep the previous night, and 0.2\n",
    "    if not.\n",
    "\n",
    "-   The probability of having red eyes is 0.2 if the student got enough\n",
    "    sleep, and 0.7 if not.\n",
    "\n",
    "-   The probability of sleeping in class is 0.1 if the student got\n",
    "    enough sleep, and 0.3 if not.\n",
    "\n",
    "Formulate this information as a dynamic Bayesian network that the\n",
    "professor could use to filter or predict from a sequence of\n",
    "observations. Then reformulate it as a hidden Markov model that has only\n",
    "a single observation variable. Give the complete probability tables for\n",
    "the model.\n",
    "\n",
    "**15.17** For the DBN specified in Exercise [sleep1-exercise](#/) and\n",
    "for the evidence values\n",
    "\n",
    "$$\n",
    "\\textbf{e}_1 = not\\space red\\space eyes,\\space not\\space sleeping\\space in\\space class\n",
    "$$\n",
    "$$\n",
    "\\textbf{e}_2 = red\\space eyes,\\space not\\space sleeping\\space in\\space class\n",
    "$$\n",
    "$$\n",
    "\\textbf{e}_3 = red\\space eyes,\\space sleeping\\space in\\space class\n",
    "$$\n",
    "\n",
    "perform the following computations:\n",
    "\n",
    "1.  State estimation: Compute $P({EnoughSleep}_t | \\textbf{e}_{1:t})$ for each\n",
    "    of $t = 1,2,3$.\n",
    "\n",
    "2.  Smoothing: Compute $P({EnoughSleep}_t | \\textbf{e}_{1:3})$ for each of\n",
    "    $t = 1,2,3$.\n",
    "\n",
    "3.  Compare the filtered and smoothed probabilities for $t=1$ and $t=2$.\n",
    "\n",
    "**15.18** Suppose that a particular student shows up with red eyes and sleeps in\n",
    "class every day. Given the model described in\n",
    "Exercise [sleep1-exercise](#/), explain why the probability\n",
    "that the student had enough sleep the previous night converges to a\n",
    "fixed point rather than continuing to go down as we gather more days of\n",
    "evidence. What is the fixed point? Answer this both numerically (by\n",
    "computation) and analytically.\n",
    "\n",
    "**15.19** \\[battery-sequence-exercise\\] This exercise analyzes in more detail the\n",
    "persistent-failure model for the battery sensor in\n",
    "Figure [battery-persistence-figure](#/)(a)\n",
    "(page [battery-persistence-figure](#/)).\n",
    "\n",
    "1.  Figure [battery-persistence-figure](#/)(b) stops at\n",
    "    $t{{\\,{=}\\,}}{32}$. Describe qualitatively what should happen as\n",
    "    $t\\to\\infty$ if the sensor continues to read 0.\n",
    "\n",
    "2.  Suppose that the external temperature affects the battery sensor in\n",
    "    such a way that transient failures become more likely as\n",
    "    temperature increases. Show how to augment the DBN structure in\n",
    "    Figure [battery-persistence-figure](#/)(a), and explain\n",
    "    any required changes to the CPTs.\n",
    "\n",
    "3.  Given the new network structure, can battery readings be used by the\n",
    "    robot to infer the current temperature?\n",
    "\n",
    "**15.20** \\[dbn-elimination-exercise\\] Consider applying the variable elimination\n",
    "algorithm to the umbrella DBN unrolled for three slices, where the query\n",
    "is ${\\textbf{P}}(R_3|u_1,u_2,u_3)$. Show that the space\n",
    "complexity of the algorithm—the size of the largest factor—is the same,\n",
    "regardless of whether the rain variables are eliminated in forward or\n",
    "backward order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
