{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**22.1** This exercise explores the quality of the $n$-gram model of language.\n",
    "Find or create a monolingual corpus of 100,000 words or more. Segment it\n",
    "into words, and compute the frequency of each word. How many distinct\n",
    "words are there? Also count frequencies of bigrams (two consecutive\n",
    "words) and trigrams (three consecutive words). Now use those frequencies\n",
    "to generate language: from the unigram, bigram, and trigram models, in\n",
    "turn, generate a 100-word text by making random choices according to the\n",
    "frequency counts. Compare the three generated texts with actual\n",
    "language. Finally, calculate the perplexity of each model.\n",
    "\n",
    "**22.2** Write a program to do **segmentation** of\n",
    "words without spaces. Given a string, such as the URL\n",
    "“thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,”\n",
    "return a list of component words: \\[“the,” “longest,” “list,”\n",
    "$\\ldots$\\]. This task is useful for parsing URLs, for spelling\n",
    "correction when words runtogether, and for languages such as Chinese\n",
    "that do not have spaces between words. It can be solved with a unigram\n",
    "or bigram word model and a dynamic programming algorithm similar to the\n",
    "Viterbi algorithm.\n",
    "\n",
    "**22.3** *Zipf’s law* of word distribution states the following:\n",
    "Take a large corpus of text, count the frequency of every word in the\n",
    "corpus, and then rank these frequencies in decreasing order. Let $f_{I}$\n",
    "be the $I$th largest frequency in this list; that is, $f_{1}$ is the\n",
    "frequency of the most common word (usually “the”), $f_{2}$ is the\n",
    "frequency of the second most common word, and so on. Zipf’s law states\n",
    "that $f_{I}$ is approximately equal to $\\alpha / I$ for some constant\n",
    "$\\alpha$. The law tends to be highly accurate except for very small and\n",
    "very large values of $I$.\n",
    "\n",
    "**22.4** Choose a corpus of at least 20,000 words of online text, and verify\n",
    "Zipf’s law experimentally. Define an error measure and find the value of\n",
    "$\\alpha$ where Zipf’s law best matches your experimental data. Create a\n",
    "log–log graph plotting $f_{I}$ vs. $I$ and $\\alpha/I$ vs. $I$. (On a\n",
    "log–log graph, the function $\\alpha/I$ is a straight line.) In carrying\n",
    "out the experiment, be sure to eliminate any formatting tokens (e.g.,\n",
    "HTML tags) and normalize upper and lower case.\n",
    "\n",
    "**22.5** (Adapted from @Jurafsky+Martin:2000.) In this exercise you will develop a classifier for\n",
    "authorship: given a text, the classifier predicts which of two candidate\n",
    "authors wrote the text. Obtain samples of text from two different\n",
    "authors. Separate them into training and test sets. Now train a language\n",
    "model on the training set. You can choose what features to use;\n",
    "$n$-grams of words or letters are the easiest, but you can add\n",
    "additional features that you think may help. Then compute the\n",
    "probability of the text under each language model and chose the most\n",
    "probable model. Assess the accuracy of this technique. How does accuracy\n",
    "change as you alter the set of features? This subfield of linguistics is\n",
    "called **stylometry**; its successes include the identification of the author of the\n",
    "disputed *Federalist Papers* @Mosteller+Wallace:1964 and\n",
    "some disputed works of Shakespeare @Hope:1994. @Khmelev+Tweedie:2001 produce good results with\n",
    "a simple letter bigram model.\n",
    "\n",
    "**22.6** This exercise concerns the classification of spam email.\n",
    "Create a corpus of spam email and one of non-spam mail. Examine each\n",
    "corpus and decide what features appear to be useful for classification:\n",
    "unigram words? bigrams? message length, sender, time of arrival? Then\n",
    "train a classification algorithm (decision tree, naive Bayes, SVM,\n",
    "logistic regression, or some other algorithm of your choosing) on a\n",
    "training set and report its accuracy on a test set.\n",
    "\n",
    "**22.7** Create a test set of ten queries, and pose them to three major Web\n",
    "search engines. Evaluate each one for precision at 1, 3, and 10\n",
    "documents. Can you explain the differences between engines?\n",
    "\n",
    "**22.8** Try to ascertain which of the search engines from the previous exercise\n",
    "are using case folding, stemming, synonyms, and spelling correction.\n",
    "\n",
    "**22.9** Estimate how much storage space is necessary for the index to a 100\n",
    "billion-page corpus of Web pages. Show the assumptions you made.\n",
    "\n",
    "**22.10** Write a regular expression or a short program to extract company names.\n",
    "Test it on a corpus of business news articles. Report your recall and\n",
    "precision.\n",
    "\n",
    "**22.11** Consider the problem of trying to evaluate the quality of an IR system\n",
    "that returns a ranked list of answers (like most Web search engines).\n",
    "The appropriate measure of quality depends on the presumed model of what\n",
    "the searcher is trying to achieve, and what strategy she employs. For\n",
    "each of the following models, propose a corresponding numeric measure.\n",
    "\n",
    "1.  The searcher will look at the first twenty answers returned, with\n",
    "    the objective of getting as much relevant information as possible.\n",
    "\n",
    "2.  The searcher needs only one relevant document, and will go down the\n",
    "    list until she finds the first one.\n",
    "\n",
    "3.  The searcher has a fairly narrow query and is able to examine all\n",
    "    the answers retrieved. She wants to be sure that she has seen\n",
    "    everything in the document collection that is relevant to her query.\n",
    "    (E.g., a lawyer wants to be sure that she has found\n",
    "    *all* relevant precedents, and is willing to spend\n",
    "    considerable resources on that.)\n",
    "\n",
    "4.  The searcher needs just one document relevant to the query, and can\n",
    "    afford to pay a research assistant for an hour’s work looking\n",
    "    through the results. The assistant can look through 100 retrieved\n",
    "    documents in an hour. The assistant will charge the searcher for the\n",
    "    full hour regardless of whether he finds it immediately or at the\n",
    "    end of the hour.\n",
    "\n",
    "5.  The searcher will look through all the answers. Examining a document\n",
    "    has cost \\$$A$; finding a relevant document has value \\$$B$; failing\n",
    "    to find a relevant document has cost \\$$C$ for each relevant\n",
    "    document not found.\n",
    "\n",
    "6.  The searcher wants to collect as many relevant documents as\n",
    "    possible, but needs steady encouragement. She looks through the\n",
    "    documents in order. If the documents she has looked at so far are\n",
    "    mostly good, she will continue; otherwise, she will stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
